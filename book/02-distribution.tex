\chapter{Distributions}
\label{descriptive}


\section{Histograms}
\label{histograms}

One of the best ways to describe a variable is to report the values
that appear in the dataset and how many times each value appears.
This description is called the {\bf distribution} of the variable.
\index{distribution}

The most common representation of a distribution is a {\bf histogram},
which is a graph that shows the {\bf frequency} of each value.  In
this context, ``frequency'' means the number of times the value
appears.  \index{histogram} \index{frequency}
\index{dictionary}

In Python, an efficient way to compute frequencies is with a
dictionary.  Given a sequence of values, {\tt t}:
%
\begin{verbatim}
hist = {}
for x in t:
    hist[x] = hist.get(x, 0) + 1
\end{verbatim}

The result is a dictionary that maps from values to frequencies.
Alternatively, you could use the {\tt Counter} class defined in the
{\tt collections} module:

\begin{verbatim}
from collections import Counter
counter = Counter(t)
\end{verbatim}

The result is a {\tt Counter} object, which is a subclass of
dictionary.

Another option is to use the pandas method \verb"value_counts", which
we saw in the previous chapter.  But for this book I created a class,
Hist, that represents histograms and provides the methods
that operate on them.
\index{pandas}


\section{Representing histograms}
\index{histogram}
\index{Hist}

The Hist constructor can take a sequence, dictionary, pandas
Series, or another Hist.  You can instantiate a Hist object like this:
%
\begin{verbatim}
>>> import thinkstats2
>>> hist = thinkstats2.Hist([1, 2, 2, 3, 5])
>>> hist
Hist({1: 1, 2: 2, 3: 1, 5: 1})
\end{verbatim}

Hist objects provide {\tt Freq}, which takes a value and
returns its frequency: \index{frequency}
%
\begin{verbatim}
>>> hist.Freq(2)
2
\end{verbatim}

The bracket operator does the same thing: \index{bracket operator}
%
\begin{verbatim}
>>> hist[2]
2
\end{verbatim}

If you look up a value that has never appeared, the frequency is 0.
%
\begin{verbatim}
>>> hist.Freq(4)
0
\end{verbatim}

{\tt Values} returns an unsorted list of the values in the Hist:
%
\begin{verbatim}
>>> hist.Values()
[1, 5, 3, 2]
\end{verbatim}

To loop through the values in order, you can use the built-in function
{\tt sorted}:
%
\begin{verbatim}
for val in sorted(hist.Values()):
    print(val, hist.Freq(val))
\end{verbatim}

Or you can use {\tt Items} to iterate through
value-frequency pairs: \index{frequency}
%
\begin{verbatim}
for val, freq in hist.Items():
     print(val, freq)
\end{verbatim}


\section{Plotting histograms}
\index{pyplot}

\begin{figure}
% first.py
%\centerline{\includegraphics[height=2.5in]{figs/first_wgt_lb_hist.pdf}}
\caption{Histogram of the pound part of birth weight.}
\label{first_wgt_lb_hist}
\end{figure}

For this book I wrote a module called {\tt thinkplot.py} that provides
functions for plotting Hists and other objects defined in {\tt
  thinkstats2.py}.  It is based on {\tt pyplot}, which is part of the
{\tt matplotlib} package.  See Section~\ref{code} for information
about installing {\tt matplotlib}.  \index{thinkplot}
\index{matplotlib}

To plot {\tt hist} with {\tt thinkplot}, try this:
\index{Hist}

\begin{verbatim}
>>> import thinkplot
>>> thinkplot.Hist(hist)
>>> thinkplot.Show(xlabel='value', ylabel='frequency')
\end{verbatim}

You can read the documentation for {\tt thinkplot} at
\url{http://greenteapress.com/thinkstats2/thinkplot.html}.


\begin{figure}
% first.py
%\centerline{\includegraphics[height=2.5in]{figs/first_wgt_oz_hist.pdf}}
\caption{Histogram of the ounce part of birth weight.}
\label{first_wgt_oz_hist}
\end{figure}


\section{NSFG variables}

Now let's get back to the data from the NSFG.  The code in this
chapter is in {\tt first.py}.  
For information about downloading and
working with this code, see Section~\ref{code}.

When you start working with a new dataset, I suggest you explore
the variables you are planning to use one at a time, and a good
way to start is by looking at histograms.
\index{histogram}

In Section~\ref{cleaning} we transformed {\tt agepreg}
from centiyears to years, and combined \verb"birthwgt_lb" and
\verb"birthwgt_oz" into a single quantity, \verb"totalwgt_lb".
In this section I use these variables to demonstrate some
features of histograms.

\begin{figure}
% first.py
%\centerline{\includegraphics[height=2.5in]{figs/first_agepreg_hist.pdf}}
\caption{Histogram of mother's age at end of pregnancy.}
\label{first_agepreg_hist}
\end{figure}

I'll start by reading the data and selecting records for live
births:

\begin{verbatim}
    preg = nsfg.ReadFemPreg()
    live = preg[preg.outcome == 1]
\end{verbatim}

The expression in brackets is a boolean Series that
selects rows from the DataFrame and returns a new DataFrame.
Next I generate and plot the histogram of
\verb"birthwgt_lb" for live births.
\index{DataFrame}
\index{Series}
\index{Hist}
\index{bracket operator}
\index{boolean}

\begin{verbatim}
    hist = thinkstats2.Hist(live.birthwgt_lb, label='birthwgt_lb')
    thinkplot.Hist(hist)
    thinkplot.Show(xlabel='pounds', ylabel='frequency')
\end{verbatim}

When the argument passed to Hist is a pandas Series, any
{\tt nan} values are dropped.  {\tt label} is a string that appears
in the legend when the Hist is plotted.
\index{pandas}
\index{Series}
\index{thinkplot}
\index{NaN}

\begin{figure}
% first.py
%\centerline{\includegraphics[height=2.5in]{figs/first_prglngth_hist.pdf}}
\caption{Histogram of pregnancy length in weeks.}
\label{first_prglngth_hist}
\end{figure}

Figure~\ref{first_wgt_lb_hist} shows the result.  The most common
value, called the {\bf mode}, is 7 pounds.  The distribution is
approximately bell-shaped, which is the shape of the {\bf normal}
distribution, also called a {\bf Gaussian} distribution.  But unlike a
true normal distribution, this distribution is asymmetric; it has
a {\bf tail} that extends farther to the left than to the right.

Figure~\ref{first_wgt_oz_hist} shows the histogram of
\verb"birthwgt_oz", which is the ounces part of birth weight.  In
theory we expect this distribution to be {\bf uniform}; that is, all
values should have the same frequency.  In fact, 0 is more common than
the other values, and 1 and 15 are less common, probably because
respondents round off birth weights that are close to an integer
value.
\index{birth weight}
\index{weight!birth}

Figure~\ref{first_agepreg_hist} shows the histogram of \verb"agepreg",
the mother's age at the end of pregnancy.  The mode is 21 years.  The
distribution is very roughly bell-shaped, but in this case the tail
extends farther to the right than left; most mothers are in
their 20s, fewer in their 30s.

Figure~\ref{first_prglngth_hist} shows the histogram of
\verb"prglngth", the length of the pregnancy in weeks.  By far the
most common value is 39 weeks.  The left tail is longer than the
right; early babies are common, but pregnancies seldom go past 43
weeks, and doctors often intervene if they do.
\index{pregnancy length}


\section{Outliers}

Looking at histograms, it is easy to identify the most common
values and the shape of the distribution, but rare values are
not always visible.
\index{histogram}

Before going on, it is a good idea to check for {\bf
  outliers}, which are extreme values that might be errors in
measurement and recording, or might be accurate reports of rare
events.
\index{outlier}

Hist provides methods {\tt Largest} and {\tt Smallest}, which take
an integer {\tt n} and return the {\tt n} largest or smallest
values from the histogram:
\index{Hist}

\begin{verbatim}
    for weeks, freq in hist.Smallest(10):
        print(weeks, freq)
\end{verbatim}

In the list of pregnancy lengths for live births, the 10 lowest values
are {\tt [0, 4, 9, 13, 17, 18, 19, 20, 21, 22]}.  Values below 10 weeks
are certainly errors; the most likely explanation is that the outcome
was not coded correctly.  Values higher than 30 weeks are probably
legitimate.  Between 10 and 30 weeks, it is hard to be sure; some
values are probably errors, but some represent premature babies.
\index{pregnancy length}

On the other end of the range, the highest values are:
%
\begin{verbatim}
weeks  count
43     148
44     46
45     10
46     1
47     1
48     7
50     2
\end{verbatim}

Most doctors recommend induced labor if a pregnancy exceeds 42 weeks,
so some of the longer values are surprising.  In particular, 50 weeks
seems medically unlikely.

The best way to handle outliers depends on ``domain knowledge'';
that is, information about where the data come from and what they
mean.  And it depends on what analysis you are planning to perform.
\index{outlier}

In this example, the motivating question is whether first babies
tend to be early (or late).  When people ask this question, they are
usually interested in full-term pregnancies, so for this analysis
I will focus on pregnancies longer than 27 weeks.


\section{First babies}

Now we can compare the distribution of pregnancy lengths for first
babies and others.  I divided the DataFrame of live births using
{\tt birthord}, and computed their histograms:
\index{DataFrame}
\index{Hist}
\index{pregnancy length}

\begin{verbatim}
    firsts = live[live.birthord == 1]
    others = live[live.birthord != 1]

    first_hist = thinkstats2.Hist(firsts.prglngth)
    other_hist = thinkstats2.Hist(others.prglngth)
\end{verbatim}

Then I plotted their histograms on the same axis:

\begin{verbatim}
    width = 0.45
    thinkplot.PrePlot(2)
    thinkplot.Hist(first_hist, align='right', width=width)
    thinkplot.Hist(other_hist, align='left', width=width)
    thinkplot.Show(xlabel='weeks', ylabel='frequency')
\end{verbatim}

{\tt thinkplot.PrePlot} takes the number of histograms
we are planning to plot; it uses this information to choose
an appropriate collection of colors.
\index{thinkplot}

\begin{figure}
% first.py
%\centerline{\includegraphics[height=2.5in]{figs/first_nsfg_hist.pdf}}
\caption{Histogram of pregnancy lengths.}
\label{first_nsfg_hist}
\end{figure}

{\tt thinkplot.Hist} normally uses {\tt align='center'} so that
each bar is centered over its value.  For this figure, I use
{\tt align='right'} and {\tt align='left'} to place
corresponding bars on either side of the value.
\index{Hist}

With {\tt width=0.45}, the total width of the two bars is 0.9,
leaving some space between each pair.

Finally, I adjust the axis to show only data between 27 and 46 weeks.
Figure~\ref{first_nsfg_hist} shows the result.
\index{pregnancy length}
\index{length!pregnancy}

Histograms are useful because they make the most frequent values
immediately apparent.  But they are not the best choice for comparing
two distributions.  In this example, there are fewer ``first babies''
than ``others,'' so some of the apparent differences in the histograms
are due to sample sizes.  In the next chapter we address this problem
using probability mass functions.


\section{Summarizing distributions}
\label{mean}

A histogram is a complete description of the distribution of a sample;
that is, given a histogram, we could reconstruct the values in the
sample (although not their order).

If the details of the distribution are important, it might be
necessary to present a histogram.  But often we want to
summarize the distribution with a few descriptive statistics.

Some of the characteristics we might want to report are:

\begin{itemize}

\item central tendency: Do the values tend to cluster around
a particular point?
\index{central tendency}

\item modes: Is there more than one cluster?
\index{mode}

\item spread: How much variability is there in the values?
\index{spread}

\item tails: How quickly do the probabilities drop off as we
move away from the modes?
\index{tail}

\item outliers: Are there extreme values far from the modes?
\index{outlier}

\end{itemize}

Statistics designed to answer these questions are called {\bf summary
  statistics}.  By far the most common summary statistic is the {\bf
  mean}, which is meant to describe the central tendency of the
distribution.  \index{mean} \index{average} \index{summary statistic}

If you have a sample of {\tt n} values, $x_i$, the mean, $\xbar$, is
the sum of the values divided by the number of values; in other words
%
\[ \xbar = \frac{1}{n} \sum_i x_i \]
%
The words ``mean'' and ``average'' are sometimes used interchangeably,
but I make this distinction:

\begin{itemize}

\item The ``mean'' of a sample is the summary statistic computed with
  the previous formula.

\item An ``average'' is one of several summary statistics you might
  choose to describe a central tendency.
\index{central tendency}

\end{itemize}

Sometimes the mean is a good description of a set of values.  For
example, apples are all pretty much the same size (at least the ones
sold in supermarkets).  So if I buy 6 apples and the total weight is 3
pounds, it would be a reasonable summary to say they are about a half
pound each.
\index{weight!pumpkin}

But pumpkins are more diverse.  Suppose I grow several varieties in my
garden, and one day I harvest three decorative pumpkins that are 1
pound each, two pie pumpkins that are 3 pounds each, and one Atlantic
Giant\textregistered~pumpkin that weighs 591 pounds.  The mean of this
sample is 100 pounds, but if I told you ``The average pumpkin in my
garden is 100 pounds,'' that would be misleading.  In this example,
there is no meaningful average because there is no typical pumpkin.
\index{pumpkin}



\section{Variance}
\index{variance}

If there is no single number that summarizes pumpkin weights,
we can do a little better with two numbers: mean and {\bf variance}.

Variance is a summary statistic intended to describe the variability
or spread of a distribution.  The variance of a set of values is
%
\[ S^2 = \frac{1}{n} \sum_i (x_i - \xbar)^2 \]
%
The term $x_i - \xbar$ is called the ``deviation from the mean,'' so
variance is the mean squared deviation.  The square root of variance,
$S$, is the {\bf standard deviation}.  \index{deviation}
\index{standard deviation}
\index{deviation}

If you have prior experience, you might have seen a formula for
variance with $n-1$ in the denominator, rather than {\tt n}.  This
statistic is used to estimate the variance in a population using a
sample.  We will come back to this in Chapter~\ref{estimation}.
\index{sample variance}

Pandas data structures provides methods to compute mean, variance and
standard deviation:
\index{pandas}

\begin{verbatim}
    mean = live.prglngth.mean()
    var = live.prglngth.var()
    std = live.prglngth.std()
\end{verbatim}

For all live births, the mean pregnancy length is 38.6 weeks, the
standard deviation is 2.7 weeks, which means we should expect
deviations of 2-3 weeks to be common.
\index{pregnancy length}

Variance of pregnancy length is 7.3, which is hard to interpret,
especially since the units are weeks$^2$, or ``square weeks.''
Variance is useful in some calculations, but it is not
a good summary statistic.


\section{Effect size}
\index{effect size}

An {\bf effect size} is a summary statistic intended to describe (wait
for it) the size of an effect.  For example, to describe the
difference between two groups, one obvious choice is the difference in
the means.  \index{effect size}

Mean pregnancy length for first babies is 38.601; for
other babies it is 38.523.  The difference is 0.078 weeks, which works
out to 13 hours.  As a fraction of the typical pregnancy length, this
difference is about 0.2\%.
\index{pregnancy length}

If we assume this estimate is accurate, such a difference
would have no practical consequences.  In fact, without
observing a large number of pregnancies, it is unlikely that anyone
would notice this difference at all.
\index{effect size}

Another way to convey the size of the effect is to compare the
difference between groups to the variability within groups.
Cohen's $d$ is a statistic intended to do that; it is defined
%
\[ d = \frac{\bar{x_1} - \bar{x_2}}{s}  \]
%
where $\bar{x_1}$ and $\bar{x_2}$ are the means of the groups and
$s$ is the ``pooled standard deviation''.  Here's the Python
code that computes Cohen's $d$:
\index{standard deviation!pooled}

\begin{verbatim}
def CohenEffectSize(group1, group2):
    diff = group1.mean() - group2.mean()

    var1 = group1.var()
    var2 = group2.var()
    n1, n2 = len(group1), len(group2)

    pooled_var = (n1 * var1 + n2 * var2) / (n1 + n2)
    d = diff / math.sqrt(pooled_var)
    return d
\end{verbatim}

In this example, the difference in means is 0.029 standard deviations,
which is small.  To put that in perspective, the difference in
height between men and women is about 1.7 standard deviations (see
\url{https://en.wikipedia.org/wiki/Effect_size}).


\section{Reporting results}

We have seen several ways to describe the difference in pregnancy
length (if there is one) between first babies and others.  How should
we report these results?
\index{pregnancy length}

The answer depends on who is asking the question.  A scientist might
be interested in any (real) effect, no matter how small.  A doctor
might only care about effects that are {\bf clinically significant};
that is, differences that affect treatment decisions.  A pregnant
woman might be interested in results that are relevant to her, like
the probability of delivering early or late.
\index{clinically significant} \index{significant}

How you report results also depends on your goals.  If you are trying
to demonstrate the importance of an effect, you might choose summary
statistics that emphasize differences.  If you are trying to reassure
a patient, you might choose statistics that put the differences in
context.

Of course your decisions should also be guided by professional ethics.
It's ok to be persuasive; you {\em should} design statistical reports
and visualizations that tell a story clearly.  But you should also do
your best to make your reports honest, and to acknowledge uncertainty
and limitations.
\index{ethics}


\section{Exercises}

\begin{exercise}
Based on the results in this chapter, suppose you were asked to
summarize what you learned about whether first babies arrive late.

Which summary statistics would you use if you wanted to get a story
on the evening news?  Which ones would you use if you wanted to
reassure an anxious patient?
\index{Adams, Cecil}
\index{Straight Dope, The}

Finally, imagine that you are Cecil Adams, author of {\it The Straight
  Dope} (\url{http://straightdope.com}), and your job is to answer the
question, ``Do first babies arrive late?''  Write a paragraph that
uses the results in this chapter to answer the question clearly,
precisely, and honestly.
\index{ethics}

\end{exercise}

\begin{exercise}
In the repository you downloaded, you should find a file named
\verb"chap02ex.ipynb"; open it.  Some cells are already filled in, and
you should execute them.  Other cells give you instructions for
exercises.  Follow the instructions and fill in the answers.

A solution to this exercise is in \verb"chap02soln.ipynb"
\end{exercise}

For the following exercises, create a file named {\tt chap02ex.py}.
You can find a solution in \verb"chap02soln.py".

\begin{exercise}
The mode of a distribution is the most frequent value; see
\url{http://wikipedia.org/wiki/Mode_(statistics)}.  Write a function
called {\tt Mode} that takes a Hist and returns the most
frequent value.\index{mode}
\index{Hist}

As a more challenging exercise, write a function called {\tt AllModes}
that returns a list of value-frequency pairs in descending order of
frequency.
\index{frequency}
\end{exercise}

\begin{exercise}
Using the variable \verb"totalwgt_lb", investigate whether first
babies are lighter or heavier than others.  Compute Cohen's $d$
to quantify the difference between the groups.  How does it
compare to the difference in pregnancy length?
\index{pregnancy length}
\end{exercise}


\section{Glossary}

\begin{itemize}

\item distribution: The values that appear in a sample
and the frequency of each.
\index{distribution}

\item histogram: A mapping from values to frequencies, or a graph
that shows this mapping.
\index{histogram}

\item frequency: The number of times a value appears in a sample.
\index{frequency}

\item mode: The most frequent value in a sample, or one of the
most frequent values.
\index{mode}

\item normal distribution: An idealization of a bell-shaped distribution;
also known as a Gaussian distribution. 
\index{Gaussian distribution}
\index{normal distribution}

\item uniform distribution: A distribution in which all values have
the same frequency.
\index{uniform distribution}

\item tail: The part of a distribution at the high and low extremes.
\index{tail}

\item central tendency: A characteristic of a sample or population;
intuitively, it is an average or typical value. 
\index{central tendency}

\item outlier: A value far from the central tendency.
\index{outlier}

\item spread: A measure of how spread out the values in a distribution
are.
\index{spread}

\item summary statistic: A statistic that quantifies some aspect
of a distribution, like central tendency or spread.
\index{summary statistic}

\item variance: A summary statistic often used to quantify spread.
\index{variance}

\item standard deviation: The square root of variance, also used
as a measure of spread.
\index{standard deviation}

\item effect size: A summary statistic intended to quantify the size
of an effect like a difference between groups.
\index{effect size}

\item clinically significant: A result, like a difference between groups,
that is relevant in practice.
\index{clinically significant}

\end{itemize}



