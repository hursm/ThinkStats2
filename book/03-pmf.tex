\chapter{확률 질량 함수}
\index{확률 질량 함수 (probability mass function)}

이번 장에서 사용되는 코드는 {\tt probability.py}에 있다.
코드를 다운로드하고 작업하는 것에 대한 정보는 ~\ref{code}을 참조한다.

\section{Pmf}
\index{Pmf}

분포를 표현하는 또다른 방식은 {\bf 확률 질량 함수(probability mass function)} (PMF)로 각 값을 확률로 매핑한다. 
{\bf 확률(probability)}은 표본 크기 {\tt n}의 일부로서 표현되는 빈도다. 
빈도에서 확률을 얻기 위해서, {\tt n}으로 나누는데 이를 {\bf 정규화(normalization)}라고 부른다.

\index{빈도 (frequency)}
\index{확률 (probability)}
\index{정규화 (normalization)}
\index{PMF}
\index{확률 질량 함수 (probability mass function)}

Hist가 주어지면, 각 값에 확률값을 매핑하는 딕셔너리를 만들 수 있다.
\index{Hist}

%
\begin{verbatim}
n = hist.Total()
d = {}
for x, freq in hist.Items():
    d[x] = freq / n
\end{verbatim}
%

혹은 {\tt thinkstats2}에서 제공하는 Pmf 클래스를 사용할 수도 있다.
Hist와 마찬가지로, Pmf 생성자는 판다스, 시리즈, 딕셔너리, Hist, 다른 Pmf 객체를 받을 수 있다.
다음에 간단한 리스트를 입력값으로 받는 예제가 있다.

%
\begin{verbatim}
>>> import thinkstats2
>>> pmf = thinkstats2.Pmf([1, 2, 2, 3, 5])
>>> pmf
Pmf({1: 0.2, 2: 0.4, 3: 0.2, 5: 0.2})
\end{verbatim}

Pmf는 정규화 과정을 거쳐 전체 확률값이 1이 된다.

Pmf와 Hist 객체는 많은 점에서 비슷하다; 사실, 공통 부모 클래스에서 많은 메쏘드를 상속받았다.
예를 들어, {\tt Values}와 {\tt Items} 메쏘드는 두 객체 모두에 동일한 방식으로 동작한다.
가장 큰 차이점은 Hist가 값을 정수형 계수기(integer counter)로 매핑한다는 것이고; 
Pmf는 값을 부동소수점 확률값으로 매핑한다는 것이다. 
\index{Hist}

값과 연관된 확률값을 조회하려면, {\tt Prob}를 사용한다.:
%
\begin{verbatim}
>>> pmf.Prob(2)
0.4
\end{verbatim}

꺾쇠 연산자도 동등한 기능을 한다.
\index{꺾쇠 연산자 (bracket operator)}

\begin{verbatim}
>>> pmf[2]
0.4
\end{verbatim}

기존 Pmf를 값과 연관되어 있는 확률값을 증가시킴으로써 변경할 수 있다.
%
\begin{verbatim}
>>> pmf.Incr(2, 0.2)
>>> pmf.Prob(2)
0.6
\end{verbatim}

혹은 확률에 일정량을 곱할 수도 있다.
%
\begin{verbatim}
>>> pmf.Mult(2, 0.5)
>>> pmf.Prob(2)
0.3
\end{verbatim}

만약 Pmf를 변경하면, 결과는 정규화되지 않을지도 모른다; 즉, 확률값을 다 합하면 1이 되지 않을지도 모른다.
확률값을 합한 결과를 반환하는데 사용되는 {\tt Total} 메쏘드를 호출해서 확인한다. 

%
\begin{verbatim}
>>> pmf.Total()
0.9
\end{verbatim}

다시 정규화하기 위해서, {\tt Normalize}를 호출한다:
%
\begin{verbatim}
>>> pmf.Normalize()
>>> pmf.Total()
1.0
\end{verbatim}

Pmf 객체는 {\tt Copy} 메쏘드를 제공하는데, 이를 통해서 원본에 영향을 주지않고, 사본을 만들고 변경 작업을 할 수 있다.
\index{Pmf}

이번 절에 표기법이 일관성을 갖지 않는 것처럼 보일지도 모르지만 체계가 있다;
Pmf를 클래스 명칭으로, pmf는 클래스의 인스턴스로, PMF는 확률질량함수에 대한 수학적 개념으로 각각을 표기하는데 사용한다.

\section{PMF 플롯으로 그리기}
\index{PMF}

{\tt thinkplot}은 Pmf 플롯을 그리는 두가지 방식을 제공한다.
\index{thinkplot}

\begin{itemize}

\item Pmf를 막대그래프로 그리기 위해서 {\tt thinkplot.Hist}을 사용한다.
만약 Pmf에 값의 개수가 작다면 막대그래프가 가장 유용한다.
\index{막대그래프 (bar plot)}
\index{그래프 (plot)!막대 (bar)}

\item 계단 함수로 Pmf를 그래프 그리기 위해서는, {\tt thinkplot.Pmf}을 사용할 수 있다.
만약 값이 많고, Pmf가 매끄럽다면(smooth) 탁월한 선택이 된다. 이 함수는 Hist 객체에서도 동작한다.
\index{선그래프 (line plot)}
\index{그래프 (plot)!선 (line)}
\index{Hist}
\index{Pmf}

\end{itemize}

추가로, {\tt pyplot}은 {\tt hist} 함수를 제공하는데 값(value) 시퀀스를 받아서
히스토그램을 계산하고, 그래프로 그린다.
Hist 객체를 사용하기 때문에, {\tt pyplot.hist}은 사용하지 않는다.
\index{pyplot}

\begin{figure}
% probability.py
%\centerline{\includegraphics[height=3.0in]{figs/probability_nsfg_pmf.pdf}}
\caption{PMF of pregnancy lengths for first babies and others, using
  bar graphs and step functions.}
\label{probability_nsfg_pmf}
\end{figure}
\index{임신기간 (pregnancy length)}
\index{기간 (length)!임신 (pregnancy)}

그림~\ref{probability_nsfg_pmf}은 막대그래프(왼쪽)와 계단함수(오른쪽)를 사용해서 첫째 아이와 첫째가 아닌 아이에 대한
임신기간 PMF를 보여준다.
\index{임신기간 (pregnancy length)}

히스토그램 대신에 PMF 플롯을 그려서, 표본차이로 오도되지 않고 두 분포를 비교할 수 있다.
그림을 해석하면, 첫번째 아이는 다른 아이들보다 정시(39주차)에 출산하지 않고, 늦게(41, 42주차) 출산할 것 같다.

그림~\ref{probability_nsfg_pmf}을 생성하는 코드가 다음에 있다:

\begin{verbatim}
    thinkplot.PrePlot(2, cols=2)
    thinkplot.Hist(first_pmf, align='right', width=width)
    thinkplot.Hist(other_pmf, align='left', width=width)
    thinkplot.Config(xlabel='weeks',
                     ylabel='probability',
                     axis=[27, 46, 0, 0.6])

    thinkplot.PrePlot(2)
    thinkplot.SubPlot(2)
    thinkplot.Pmfs([first_pmf, other_pmf])
    thinkplot.Show(xlabel='weeks',
                   axis=[27, 46, 0, 0.6])
\end{verbatim}

{\tt PrePlot} 메쏘드는 옵션 매개변수 {\tt rows}와 {\tt cols}을 받아서 
그림 격자(grid)를 만드는데 이경우 한 행에 그림 두개를 넣는 격자가 된다.
첫번째 그림(왼쪽)은 {\tt thinkplot.Hist}을 사용해서 앞에서 봤던 Pmf를 화면에 출력한다. 

\index{thinkplot}
\index{Hist}

{\tt PrePlot}에 두번째 호출로 색깔 생성자를 초기화한다. 그리고 나서 
{\tt SubPlot}이 두번째 그림(오른쪽)으로 바꿔서, {\tt thinkplot.Pmfs}을 사용해서 Pmf를 화면에 출력한다.
{\tt axis}을 사용해서 그림 두개 모두 동일한 축(axis)에 놓여지도록 확실히 한다.
그림 두개를 비교하려고 한다면, 축을 통일하는 것이 좋다.


\section{다른 시각화 방법}
\label{visualization}

Histograms and PMFs are useful while you are exploring data and
trying to identify patterns and relationships.
Once you have an idea what is going on, a good next step is to
design a visualization that makes the patterns you have identified
as clear as possible.
\index{exploratory data analysis}
\index{visualization}

In the NSFG data, the biggest differences in the distributions are
near the mode.  So it makes sense to zoom in on that part of the
graph, and to transform the data to emphasize differences:
\index{National Survey of Family Growth}
\index{NSFG}

\begin{verbatim}
    weeks = range(35, 46)
    diffs = []
    for week in weeks:
        p1 = first_pmf.Prob(week)
        p2 = other_pmf.Prob(week)
        diff = 100 * (p1 - p2)
        diffs.append(diff)

    thinkplot.Bar(weeks, diffs)
\end{verbatim}

In this code, {\tt weeks} is the range of weeks; {\tt diffs} is the
difference between the two PMFs in percentage points.
Figure~\ref{probability_nsfg_diffs} shows the result as a bar chart.
This figure makes the pattern clearer: first babies are less likely to
be born in week 39, and somewhat more likely to be born in weeks 41
and 42.
\index{thinkplot}

\begin{figure}
% probability.py
%\centerline{\includegraphics[height=2.5in]{figs/probability_nsfg_diffs.pdf}}
\caption{Difference, in percentage points, by week.}
\label{probability_nsfg_diffs}
\end{figure}

For now we should hold this conclusion only tentatively.
We used the same dataset to identify an
apparent difference and then chose a visualization that makes the
difference apparent.  We can't be sure this effect is real;
it might be due to random variation.  We'll address this concern
later.


\section{The class size paradox}
\index{class size}

Before we go on, I want to demonstrate
one kind of computation you can do with Pmf objects; I call
this example the ``class size paradox.''
\index{Pmf}

At many American colleges and universities, the student-to-faculty
ratio is about 10:1.  But students are often surprised to discover
that their average class size is bigger than 10.  There
are two reasons for the discrepancy:

\begin{itemize}

\item Students typically take 4--5 classes per semester, but
professors often teach 1 or 2.

\item The number of students who enjoy a small class is small,
but the number of students in a large class is (ahem!) large.

\end{itemize}

The first effect is obvious, at least once it is pointed out;
the second is more subtle.  Let's look at an example.  Suppose
that a college offers 65 classes in a given semester, with the
following distribution of sizes:
%
\begin{verbatim}
 size      count
 5- 9          8
10-14          8
15-19         14
20-24          4
25-29          6
30-34         12
35-39          8
40-44          3
45-49          2
\end{verbatim}

If you ask the Dean for the average class size, he would
construct a PMF, compute the mean, and report that the
average class size is 23.7.  Here's the code:

\begin{verbatim}
    d = { 7: 8, 12: 8, 17: 14, 22: 4, 
          27: 6, 32: 12, 37: 8, 42: 3, 47: 2 }

    pmf = thinkstats2.Pmf(d, label='actual')
    print('mean', pmf.Mean())
\end{verbatim}

But if you survey a group of students, ask them how many
students are in their classes, and compute the mean, you would
think the average class was bigger.  Let's see how
much bigger.

First, I compute the
distribution as observed by students, where the probability
associated with each class size is ``biased'' by the number
of students in the class.
\index{observer bias}
\index{bias!observer}

\begin{verbatim}
def BiasPmf(pmf, label):
    new_pmf = pmf.Copy(label=label)

    for x, p in pmf.Items():
        new_pmf.Mult(x, x)
        
    new_pmf.Normalize()
    return new_pmf
\end{verbatim}

For each class size, {\tt x}, we multiply the probability by
{\tt x}, the number of students who observe that class size.
The result is a new Pmf that represents the biased distribution.

Now we can plot the actual and observed distributions:
\index{thinkplot}

\begin{verbatim}
    biased_pmf = BiasPmf(pmf, label='observed')
    thinkplot.PrePlot(2)
    thinkplot.Pmfs([pmf, biased_pmf])
    thinkplot.Show(xlabel='class size', ylabel='PMF')
\end{verbatim}

\begin{figure}
% probability.py
%\centerline{\includegraphics[height=3.0in]{figs/class_size1.pdf}}
\caption{Distribution of class sizes, actual and as observed by students.}
\label{class_size1}
\end{figure}

Figure~\ref{class_size1} shows the result.  In the biased distribution
there are fewer small classes and more large ones.
The mean of the biased distribution is 29.1, almost 25\% higher
than the actual mean.

It is also possible to invert this operation.  Suppose you want to
find the distribution of class sizes at a college, but you can't get
reliable data from the Dean.  An alternative is to choose a random
sample of students and ask how many students are in their
classes.  \index{bias!oversampling} \index{oversampling}

The result would be biased for the reasons we've just seen, but you
can use it to estimate the actual distribution.  Here's the function
that unbiases a Pmf:

\begin{verbatim}
def UnbiasPmf(pmf, label):
    new_pmf = pmf.Copy(label=label)

    for x, p in pmf.Items():
        new_pmf.Mult(x, 1.0/x)
        
    new_pmf.Normalize()
    return new_pmf
\end{verbatim}

It's similar to {\tt BiasPmf}; the only difference is that it
divides each probability by {\tt x} instead of multiplying.


\section{DataFrame indexing}

In Section~\ref{dataframe} we read a pandas DataFrame and used it to
select and modify data columns.  Now let's look at row selection.
To start, I create a NumPy array of random numbers and use it
to initialize a DataFrame:
\index{NumPy}
\index{pandas}
\index{DataFrame}

\begin{verbatim}
>>> import numpy as np
>>> import pandas
>>> array = np.random.randn(4, 2)
>>> df = pandas.DataFrame(array)
>>> df
          0         1
0 -0.143510  0.616050
1 -1.489647  0.300774
2 -0.074350  0.039621
3 -1.369968  0.545897
\end{verbatim}

By default, the rows and columns are numbered starting at zero, but
you can provide column names:

\begin{verbatim}
>>> columns = ['A', 'B']
>>> df = pandas.DataFrame(array, columns=columns)
>>> df
          A         B
0 -0.143510  0.616050
1 -1.489647  0.300774
2 -0.074350  0.039621
3 -1.369968  0.545897
\end{verbatim}

You can also provide row names.  The set of row names is called the
{\bf index}; the row names themselves are called {\bf labels}.

\begin{verbatim}
>>> index = ['a', 'b', 'c', 'd']
>>> df = pandas.DataFrame(array, columns=columns, index=index)
>>> df
          A         B
a -0.143510  0.616050
b -1.489647  0.300774
c -0.074350  0.039621
d -1.369968  0.545897
\end{verbatim}

As we saw in the previous chapter, simple indexing selects a
column, returning a Series:
\index{Series}

\begin{verbatim}
>>> df['A']
a   -0.143510
b   -1.489647
c   -0.074350
d   -1.369968
Name: A, dtype: float64
\end{verbatim}

To select a row by label, you can use the {\tt loc} attribute, which
returns a Series:

\begin{verbatim}
>>> df.loc['a']
A   -0.14351
B    0.61605
Name: a, dtype: float64
\end{verbatim}

If you know the integer position of a row, rather than its label, you
can use the {\tt iloc} attribute, which also returns a Series.

\begin{verbatim}
>>> df.iloc[0]
A   -0.14351
B    0.61605
Name: a, dtype: float64
\end{verbatim}

{\tt loc} can also take a list of labels; in that case,
the result is a DataFrame.

\begin{verbatim}
>>> indices = ['a', 'c']
>>> df.loc[indices]
         A         B
a -0.14351  0.616050
c -0.07435  0.039621
\end{verbatim}

Finally, you can use a slice to select a range of rows by label:

\begin{verbatim}
>>> df['a':'c']
          A         B
a -0.143510  0.616050
b -1.489647  0.300774
c -0.074350  0.039621
\end{verbatim}

Or by integer position:

\begin{verbatim}
>>> df[0:2]
          A         B
a -0.143510  0.616050
b -1.489647  0.300774
\end{verbatim}

The result in either case is a DataFrame, but notice that the first
result includes the end of the slice; the second doesn't.
\index{DataFrame}

My advice: if your rows have labels that are not simple integers, use
the labels consistently and avoid using integer positions.



\section{Exercises}

Solutions to these exercises are in \verb"chap03soln.ipynb"
and \verb"chap03soln.py"

\begin{exercise}
Something like the class size paradox appears if you survey children
and ask how many children are in their family.  Families with many
children are more likely to appear in your sample, and
families with no children have no chance to be in the sample.
\index{observer bias}
\index{bias!observer}

Use the NSFG respondent variable \verb"NUMKDHH" to construct the actual
distribution for the number of children under 18 in the household.

Now compute the biased distribution we would see if we surveyed the
children and asked them how many children under 18 (including themselves)
are in their household.  

Plot the actual and biased distributions, and compute their means.
As a starting place, you can use \verb"chap03ex.ipynb".
\end{exercise}


\begin{exercise}
\index{mean}
\index{variance}
\index{PMF}

In Section~\ref{mean} we computed the mean of a sample by adding up
the elements and dividing by n.  If you are given a PMF, you can
still compute the mean, but the process is slightly different:
%
\[ \xbar = \sum_i p_i~x_i \]
%
where the $x_i$ are the unique values in the PMF and $p_i=PMF(x_i)$.
Similarly, you can compute variance like this:
%
\[ S^2 = \sum_i p_i~(x_i - \xbar)^2\]
% 
Write functions called {\tt PmfMean} and {\tt PmfVar} that take a
Pmf object and compute the mean and variance.  To test these methods,
check that they are consistent with the methods {\tt Mean} and {\tt
  Var} provided by Pmf.
\index{Pmf}

\end{exercise}


\begin{exercise}
I started with the question, ``Are first babies more likely
to be late?''  To address it, I computed the difference in
means between groups of babies, but I ignored the possibility
that there might be a difference between first babies and
others {\em for the same woman}.

To address this version of the question, select respondents who
have at least two babies and compute pairwise differences.  Does
this formulation of the question yield a different result?

Hint: use {\tt nsfg.MakePregMap}.
\end{exercise}


\begin{exercise}
\label{relay}

In most foot races, everyone starts at the same time.  If you are a
fast runner, you usually pass a lot of people at the beginning of the
race, but after a few miles everyone around you is going at the same
speed.
\index{relay race}

When I ran a long-distance (209 miles) relay race for the first
time, I noticed an odd phenomenon: when I overtook another runner, I
was usually much faster, and when another runner overtook me, he was
usually much faster.

At first I thought that the distribution of speeds might be bimodal;
that is, there were many slow runners and many fast runners, but few
at my speed.

Then I realized that I was the victim of a bias similar to the
effect of class size.  The race
was unusual in two ways: it used a staggered start, so teams started
at different times; also, many teams included runners at different
levels of ability. \index{bias!selection} \index{selection bias}

As a result, runners were spread out along the course with little
relationship between speed and location.  When I joined the race, the
runners near me were (pretty much) a random sample of the runners in
the race.

So where does the bias come from?  During my time on the course, the
chance of overtaking a runner, or being overtaken, is proportional to
the difference in our speeds.  I am more likely to catch a slow
runner, and more likely to be caught by a fast runner.  But runners
at the same speed are unlikely to see each other.

Write a function called {\tt ObservedPmf} that takes a Pmf representing
the actual distribution of runners' speeds, and the speed of a running
observer, and returns a new Pmf representing the distribution of
runners' speeds as seen by the observer.
\index{observer bias}
\index{bias!observer}

To test your function, you can use {\tt relay.py}, which  reads the
results from the James Joyce Ramble 10K in Dedham MA and converts the
pace of each runner to mph.

Compute the distribution of speeds you would observe if you ran a
relay race at 7.5 mph with this group of runners.  A solution to this
exercise is in \verb"relay_soln.py".
\end{exercise}


\section{Glossary}

\begin{itemize}

\item Probability mass function (PMF): a representation of a distribution
as a function that maps from values to probabilities.
\index{PMF}
\index{probability mass function}

\item probability: A frequency expressed as a fraction of the sample
size.
\index{frequency}
\index{probability}

\item normalization: The process of dividing a frequency by a sample
size to get a probability.
\index{normalization}

\item index: In a pandas DataFrame, the index is a special column
that contains the row labels.
\index{pandas}
\index{DataFrame}

\end{itemize}
