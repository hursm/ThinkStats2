

\chapter{추정 (Estimation)}
\label{estimation}
\index{추정 (estimation)}

이번 장에서 사용되는 코드는 {\tt estimation.py}에 있다.
코드를 다운로드하고 작업하는 것에 대한 정보는 ~\ref{code}을 참조한다.


\section{추정 게임}

게임 한판합시다. 저자는 생각하는 분포가 있고, 여러분은 그 분포가 무엇인지 맞춰야 한다.
힌트를 두개 준다: 정규분포이고, 정규분포에서 나온 표본이 다음과 같다.

\index{정규분포 (normal distribution)}
\index{분포 (distribution)!정규 (normal)}
\index{가우스 분포 (Gaussian distribution)}
\index{분포 (distribution)!가우스 (Gaussian)}

{\tt [-0.441, 1.774, -0.101, -1.138, 2.975, -2.138]}

상기 분포의 평균 모수, $\mu$가 무얼까요?
\index{평균 (mean)}
\index{모수 (parameter)}

한가지 선택은 $\mu$ 추정값(estimate)으로 표본 평균 $\xbar$을 사용한다.
상기 예제애서 $\xbar$가 0.155 으로, $\mu$ = 0.155 추정하는 것은 합리적이다.
이러한 과정을 {\bf 추정 (estimation)}이라고 부른다. 사용한 통계량 (표본 평균)은 
{\bf 추정량(estimator)}이라고 부른다.
\index{추정량 (estimator)}

$\mu$를 추정하는데 표본 평균을 사용하는 것이 너무 명확해서 합리적인 다른 대안을 상상하기는 어렵다.
하지만, 이상값을 도입해서 게임을 바꾸는 것을 가정하자.
\index{정규분포 (normal distribution)}
\index{분포 (distribution)!정규 (normal)}
\index{가우스 분포 (Gaussian distribution)}
\index{분포 (distribution)!가우스 (Gaussian)}

{\em 저자가 생각하는 분포가 있다.} 정규분포이고, 잘못된 자리에 종종 소수점을 넣는
신뢰가 가지 않는 조사원이 수집한 표본이 다음에 있다.
\index{측정 오류 (measurement error)}

{\tt [-0.441, 1.774, -0.101, -1.138, 2.975, -213.8]}

이제 $\mu$ 추정값은 무엇이 될까요? 만약 표본 평균을 사용한다면, 추정치는 -35.12가 된다.
이것이 최선의 선택일까요? 대안이 무엇이 될까요?
\index{이상값 (outlier)}

한가지 선택지는 이상점을 식별하고 버리고 나서, 나머지를 가지고 표본 평균을 계산한다.
다른 선택지는 중위수를 추정량으로 사용한다.
\index{중위수 (median)}

어느 추정량이 가장 좋은가는 상황에 따라 달라지고(예를 들어, 이상값이 있느냐),
목적이 무엇이냐에 달려있다. 오류를 최소화하려고 하는지 혹은 정답을 얻는 가능성을 극대화하려고 하는지.

\index{오류 (error)}
\index{MSE}
\index{누적평균제곱오차 (mean squared error)}

만약 이상값이 없다면, 표본 평균은 {\bf 누적평균제곱오차 (mean squared
error), MSE}를 최소화한다. 즉, 만약 게임을 여러번하고, 매번 오차 
$\xbar - \mu$를 계산하면, 표본 평균은 다음을 최소화 한다.

%
\[ MSE = \frac{1}{m} \sum (\xbar - \mu)^2 \]
%
$m$은 추정 게임을 수행한 횟수다. $\xbar$를 계산하는데 사용된 표본 크기인 $n$와 혼동하면 안된다.

다음에 함수가 있는데 추정 게임을 모사하여 MSE 제곱근인, 
누적평균제곱오차의 제곱근(root mean squared error, RMSE)을 계산한다.

\index{누적평균제곱오차 (mean squared error)}
\index{MSE}
\index{RMSE}

\begin{verbatim}
def Estimate1(n=7, m=1000):
    mu = 0
    sigma = 1

    means = []
    medians = []
    for _ in range(m):
        xs = [random.gauss(mu, sigma) for i in range(n)]
        xbar = np.mean(xs)
        median = np.median(xs)
        means.append(xbar)
        medians.append(median)

    print('rmse xbar', RMSE(means, mu))
    print('rmse median', RMSE(medians, mu))
\end{verbatim}

다시 한번, {\tt n}은 표본 크기다. {\tt m}은 게임을 실행한 횟수다.
{\tt means}은 $\xbar$에 기반한 추정값 리스트다. 
{\tt medians}은 중위수 리스트다.
\index{중위수 (median)}

다음에 RMSE를 계산하는 함수가 있다.

\begin{verbatim}
def RMSE(estimates, actual):
    e2 = [(estimate-actual)**2 for estimate in estimates]
    mse = np.mean(e2)
    return math.sqrt(mse)
\end{verbatim}

{\tt estimates}는 추정값 리스트다; {\tt actual}은 추정되는 실제 값이다.
실무에서는 물론 {\tt actual}을 모른다; 만약 알고 있다면, 추정할 필요가 없을 것이다. 실험 목적은 두 추정량 성능을 비교하는 것이다.
\index{추정량 (estimator)}

코드를 실행하면, 표본 평균 RMSE가 0.41로 의미하는 바는 만약
$n=7$ 표본에 기반하여 $\xbar$를 사용해서 분포 평균을 추정한다면,
평균 0.41만큼 떨어졌다고 예측된다. 중위수를 사용하여 평균을 추정하면 RMSE 0.53을 산출하는데 적어도 이 사례에서 $\xbar$가 더 낮은 RMSE를 뽑아낸다고 확인해준다.

MSE 최소화는 좋은 특성이다. 하지만, 항상 최선의 전략은 되지 못한다.
예를 들어, 조선소에서 바람 속도 분포를 추정한다고 가정하다.
만약 추정값이 너무 높으면, 시설물을 과도하게 짓게되어서 비용이 상승한다.
하지만, 너무 낮게 추정한다면, 구조물이 붕괴할 수도 있다. 오류 함수로 비용이 좌우 대칭이 아니기 때문에, MSE 최소화가 항상 좋은 전략은 아니다.
\index{예측, (prediction)}
\index{비용 함수 (cost function)}
\index{MSE}

다른 예제로, 6면 주사위 세개를 던져서 합계를 예측하는 것을 가정하자.
정확하게 합계를 맞춘다면, 상을 받게 된다; 맞추지 못하면 아무 것도 없다.
이경우, MSE를 최소화하는 값은 10.5가 된다. 하지만, 좋지 못한 추정값이 된다.
왜냐하면 주사위 세개를 던져 합은 결코 10.5가 되지 못한다. 이 게임에서,
맞출 가장 높은 확률을 가진 추정량이 필요하다. 그것은 {\bf 최대우도추정량 (maximum likelihood estimator, MLE}이다. 만약 10 혹은 11일 선택하면,
우승 가능성이 8분의 1이 되고 할 수 있는 최선이 된다.

\index{MLE}
\index{최대우도추정량 (maximum likelihood estimator)}
\index{주사위 (dice)}


\section{Guess the variance}
\index{variance}
\index{normal distribution}
\index{distribution!normal}
\index{Gaussian distribution}
\index{distribution!Gaussian}

{\em I'm thinking of a distribution.}  It's a normal distribution, and 
here's a (familiar) sample:

{\tt [-0.441, 1.774, -0.101, -1.138, 2.975, -2.138]}

What do you think is the variance, $\sigma^2$, of my distribution?
Again, the obvious choice is to use the sample variance, $S^2$, as an
estimator.
%
\[ S^2 = \frac{1}{n} \sum (x_i - \xbar)^2 \] 
%
For large samples, $S^2$ is an adequate estimator, but for small
samples it tends to be too low.  Because of this unfortunate
property, it is called a {\bf biased} estimator.
An estimator is {\bf unbiased} if the expected total (or mean) error,
after many iterations of the estimation game, is 0.
\index{sample variance}
\index{biased estimator}
\index{estimator!biased}
\index{unbiased estimator}
\index{estimator!unbiased}

Fortunately, there is another simple statistic that is an unbiased
estimator of $\sigma^2$:
%
\[ S_{n-1}^2 = \frac{1}{n-1} \sum (x_i - \xbar)^2 \] 
%
For an explanation of why $S^2$ is biased, and a proof that
$S_{n-1}^2$ is unbiased, see
\url{http://wikipedia.org/wiki/Bias_of_an_estimator}.

The biggest problem with this estimator is that its name and symbol
are used inconsistently.  The name ``sample variance'' can refer to
either $S^2$ or $S_{n-1}^2$, and the symbol $S^2$ is used
for either or both.

Here is a function that simulates the estimation game and tests
the performance of $S^2$ and $S_{n-1}^2$:

\begin{verbatim}
def Estimate2(n=7, m=1000):
    mu = 0
    sigma = 1

    estimates1 = []
    estimates2 = []
    for _ in range(m):
        xs = [random.gauss(mu, sigma) for i in range(n)]
        biased = np.var(xs)
        unbiased = np.var(xs, ddof=1)
        estimates1.append(biased)
        estimates2.append(unbiased)

    print('mean error biased', MeanError(estimates1, sigma**2))
    print('mean error unbiased', MeanError(estimates2, sigma**2))
\end{verbatim}

Again, {\tt n} is the sample size and {\tt m} is the number of times
we play the game.  {\tt np.var} computes $S^2$ by default and
$S_{n-1}^2$ if you provide the argument {\tt ddof=1}, which stands for
``delta degrees of freedom.''  I won't explain that term, but you can read
about it at
\url{http://en.wikipedia.org/wiki/Degrees_of_freedom_(statistics)}.
\index{degrees of freedom}

{\tt MeanError} computes the mean difference between the estimates
and the actual value:

\begin{verbatim}
def MeanError(estimates, actual):
    errors = [estimate-actual for estimate in estimates]
    return np.mean(errors)
\end{verbatim}

When I ran this code, the mean error for $S^2$ was -0.13.  As
expected, this biased estimator tends to be too low.  For $S_{n-1}^2$,
the mean error was 0.014, about 10 times smaller.  As {\tt m}
increases, we expect the mean error for $S_{n-1}^2$ to approach 0.
\index{mean error}

Properties like MSE and bias are long-term expectations based on
many iterations of the estimation game.  By running simulations like
the ones in this chapter, we can compare estimators and check whether
they have desired properties.
\index{biased estimator}
\index{estimator!biased}

But when you apply an estimator to real
data, you just get one estimate.  It would not be meaningful to say
that the estimate is unbiased; being unbiased is a property of the
estimator, not the estimate.

After you choose an estimator with appropriate properties, and use it to
generate an estimate, the next step is to characterize the
uncertainty of the estimate, which is the topic of the next
section.


\section{Sampling distributions}
\label{gorilla}

Suppose you are a scientist studying gorillas in a wildlife
preserve.  You want to know the average weight of the adult
female gorillas in the preserve.  To weigh them, you have
to tranquilize them, which is dangerous, expensive, and possibly
harmful to the gorillas.  But if it is important to obtain this
information, it might be acceptable to weigh a sample of 9
gorillas.  Let's assume that the population of the preserve is
well known, so we can choose a representative sample of adult
females.  We could use the sample mean, $\xbar$, to estimate the
unknown population mean, $\mu$.
\index{gorilla}
\index{population}
\index{sample}

Having weighed 9 female gorillas, you might find $\xbar=90$ kg and
sample standard deviation, $S=7.5$ kg.  The sample mean
is an unbiased estimator of $\mu$, and in the long run it
minimizes MSE.  So if you report a single
estimate that summarizes the results, you would report 90 kg.
\index{MSE}
\index{sample mean}
\index{biased estimator}
\index{estimator!biased}
\index{standard deviation}

But how confident should you be in this estimate?  If you only weigh
$n=9$ gorillas out of a much larger population, you might be unlucky
and choose the 9 heaviest gorillas (or the 9 lightest ones) just by
chance.  Variation in the estimate caused by random selection is
called {\bf sampling error}.
\index{sampling error}

To quantify sampling error, we can simulate the
sampling process with hypothetical values of $\mu$ and $\sigma$, and
see how much $\xbar$ varies.

Since we don't know the actual values of 
$\mu$ and $\sigma$ in the population, we'll use the estimates
$\xbar$ and $S$.
So the question we answer is:
``If the actual values of $\mu$ and $\sigma$ were 90 kg and 7.5 kg,
and we ran the same experiment many times, how much would the
estimated mean, $\xbar$, vary?''

The following function answers that question:

\begin{verbatim}
def SimulateSample(mu=90, sigma=7.5, n=9, m=1000):
    means = []
    for j in range(m):
        xs = np.random.normal(mu, sigma, n)
        xbar = np.mean(xs)
        means.append(xbar)

    cdf = thinkstats2.Cdf(means)
    ci = cdf.Percentile(5), cdf.Percentile(95)
    stderr = RMSE(means, mu)
\end{verbatim}

{\tt mu} and {\tt sigma} are the {\em hypothetical} values of
the parameters.  {\tt n} is the sample size, the number of
gorillas we measured.  {\tt m} is the number of times we run
the simulation.
\index{gorilla}
\index{sample size}
\index{simulation}

\begin{figure}
% estimation.py
%\centerline{\includegraphics[height=2.5in]{figs/estimation1.pdf}}
\caption{Sampling distribution of $\xbar$, with confidence interval.}
\label{estimation1}
\end{figure}

In each iteration, we choose {\tt n} values from a normal
distribution with the given parameters, and compute the sample mean,
{\tt xbar}.  We run 1000 simulations and then compute the
distribution, {\tt cdf}, of the estimates.  The result is shown in
Figure~\ref{estimation1}.  This distribution is called the {\bf
  sampling distribution} of the estimator.  It shows how much the
estimates would vary if we ran the experiment over and over.
\index{sampling distribution}

The mean of the sampling distribution is pretty close
to the hypothetical value of $\mu$, which means that the experiment
yields the right answer, on average.  After 1000 tries, the lowest
result is 82 kg, and the highest is 98 kg.  This range suggests that
the estimate might be off by as much as 8 kg.

There are two common ways to summarize the sampling distribution:

\begin{itemize}

\item {\bf Standard error} (SE) is a measure of how far we expect the
  estimate to be off, on average.  For each simulated experiment, we
  compute the error, $\xbar - \mu$, and then compute the root mean
  squared error (RMSE).  In this example, it is roughly 2.5 kg.
\index{standard error}

\item A {\bf confidence interval} (CI) is a range that includes a
  given fraction of the sampling distribution.  For example, the 90\%
  confidence interval is the range from the 5th to the 95th
  percentile.  In this example, the 90\% CI is $(86, 94)$ kg.
\index{confidence interval}
\index{sampling distribution}

\end{itemize}

Standard errors and confidence intervals are the source of much confusion:

\begin{itemize}

\item People often confuse standard error and standard deviation.
  Remember that standard deviation describes variability in a measured
  quantity; in this example, the standard deviation of gorilla weight
  is 7.5 kg.  Standard error describes variability in an estimate.  In
  this example, the standard error of the mean, based on a sample of 9
  measurements, is 2.5 kg.
\index{gorilla}
\index{standard deviation}

  One way to remember the difference is that, as sample size
  increases, standard error gets smaller; standard deviation does not.

\item People often think that there is a 90\% probability that the
  actual parameter, $\mu$, falls in the 90\% confidence interval.
  Sadly, that is not true.  If you want to make a claim like that, you
  have to use Bayesian methods (see my book, {\it Think Bayes}).
\index{Bayesian statistics}

  The sampling distribution answers a different question: it gives you
  a sense of how reliable an estimate is by telling you how much it
  would vary if you ran the experiment again.
\index{sampling distribution}

\end{itemize}

It is important to remember that confidence intervals
and standard errors only quantify sampling error; that is,
error due to measuring only part of the population.
The sampling distribution does not account for other
sources of error, notably sampling bias and measurement error, 
which are the topics of the next section.


\section{Sampling bias}

Suppose that instead of the weight of gorillas in a nature preserve,
you want to know the average weight of women in the city where you
live.  It is unlikely that you would be allowed
to choose a representative sample of women and
weigh them.
\index{gorilla}
\index{adult weight}
\index{sampling bias}
\index{bias!sampling}
\index{measurement error}

A simple alternative would be
``telephone sampling;'' that is,
you could choose random numbers from the phone book, call and ask to
speak to an adult woman, and ask how much she weighs.
\index{telephone sampling}
\index{random number}

Telephone sampling has obvious limitations.  For example, the sample
is limited to people whose telephone numbers are listed, so it
eliminates people without phones (who might be poorer than average)
and people with unlisted numbers (who might be richer).  Also, if you
call home telephones during the day, you are less likely to sample
people with jobs.  And if you only sample the person who answers the
phone, you are less likely to sample people who share a phone line.

If factors like income, employment, and household size are related
to weight---and it is plausible that they are---the results of your
survey would be affected one way or another.  This problem is
called {\bf sampling bias} because it is a property of the sampling
process.
\index{sampling bias}

This sampling process is also vulnerable to self-selection, which is a
kind of sampling bias.  Some people will refuse to answer the
question, and if the tendency to refuse is related to weight, that
would affect the results.
\index{self-selection}

Finally, if you ask people how much they weigh, rather than weighing
them, the results might not be accurate.  Even helpful respondents
might round up or down if they are uncomfortable with their actual
weight.  And not all respondents are helpful.  These inaccuracies are
examples of {\bf measurement error}.
\index{measurement error}

When you report an estimated quantity, it is useful to report
standard error, or a confidence interval, or both, in order to
quantify sampling error.  But it is also important to remember that
sampling error is only one source of error, and often it is not the
biggest.
\index{standard error}
\index{confidence interval}


\section{Exponential distributions}
\index{exponential distribution}
\index{distribution!exponential}

Let's play one more round of the estimation game.
{\em I'm thinking of a distribution.}  It's an exponential distribution, and 
here's a sample:

{\tt [5.384, 4.493, 19.198, 2.790, 6.122, 12.844]}

What do you think is the parameter, $\lambda$, of this distribution?
\index{parameter}
\index{mean}

\newcommand{\lamhat}{L}
\newcommand{\lamhatmed}{L_m}

In general, the mean of an exponential distribution is $1/\lambda$,
so working backwards, we might choose
%
\[ \lamhat = 1 / \xbar\]
%
$\lamhat$ is an
estimator of $\lambda$.  And not just any estimator; it is also the
maximum likelihood estimator (see
\url{http://wikipedia.org/wiki/Exponential_distribution#Maximum_likelihood}).
So if you want to maximize your chance of guessing $\lambda$ exactly,
$\lamhat$ is the way to go.
\index{MLE}
\index{maximum likelihood estimator}

But we know that $\xbar$ is not robust in the presence of outliers, so
we expect $\lamhat$ to have the same problem.
\index{robust}
\index{outlier}
\index{sample median}

We can choose an alternative based on the sample median.
The median of an exponential distribution is $\ln(2) / \lambda$,
so working backwards again, we can define an estimator
%
\[ \lamhatmed = \ln(2) / m \]
%
where $m$ is the sample median.
\index{median}

To test the performance of these estimators, we can simulate the
sampling process:

\begin{verbatim}
def Estimate3(n=7, m=1000):
    lam = 2

    means = []
    medians = []
    for _ in range(m):
        xs = np.random.exponential(1.0/lam, n)
        L = 1 / np.mean(xs)
        Lm = math.log(2) / thinkstats2.Median(xs)
        means.append(L)
        medians.append(Lm)

    print('rmse L', RMSE(means, lam))
    print('rmse Lm', RMSE(medians, lam))
    print('mean error L', MeanError(means, lam))
    print('mean error Lm', MeanError(medians, lam))
\end{verbatim}

When I run this experiment with $\lambda=2$, the RMSE of $L$ is
1.1.  For the median-based estimator $L_m$, RMSE is 1.8.  We can't
tell from this experiment whether $L$ minimizes MSE, but at least
it seems better than $L_m$.
\index{MSE}
\index{RMSE}

Sadly, it seems that both estimators are biased.  For $L$ the mean
error is 0.33; for $L_m$ it is 0.45.  And neither converges to 0
as {\tt m} increases.
\index{biased estimator}
\index{estimator!biased}

It turns out that $\xbar$ is an unbiased estimator of the mean
of the distribution, $1 / \lambda$, but $L$ is not an unbiased
estimator of $\lambda$.


\section{Exercises}

For the following exercises, you might want to start with a copy of
{\tt estimation.py}.  Solutions are in \verb"chap08soln.py"

\begin{exercise}

In this chapter we used $\xbar$ and median to estimate $\mu$, and
found that $\xbar$  yields lower MSE.
Also, we used $S^2$ and $S_{n-1}^2$ to estimate $\sigma$, and found that
$S^2$ is biased and $S_{n-1}^2$ unbiased.

Run similar experiments to see if $\xbar$ and median are biased estimates
of $\mu$.
Also check whether $S^2$ or $S_{n-1}^2$ yields a lower MSE.
\index{sample mean}
\index{sample median}
\index{estimator!biased}

\end{exercise}


\begin{exercise}

Suppose you draw a sample with size $n=10$ from 
an exponential distribution with $\lambda=2$.  Simulate
this experiment 1000 times and plot the sampling distribution of
the estimate $\lamhat$.  Compute the standard error of the estimate
and the 90\% confidence interval.
\index{standard error}
\index{confidence interval}
\index{sampling distribution}

Repeat the experiment with a few different values of $n$ and make
a plot of standard error versus $n$.
\index{exponential distribution}
\index{distribution!exponential}


\end{exercise}


\begin{exercise}

In games like hockey and soccer, the time between goals is
roughly exponential.  So you could estimate a team's goal-scoring rate
by observing the number of goals they score in a game.  This
estimation process is a little different from sampling the time
between goals, so let's see how it works.
\index{hockey}
\index{soccer}

Write a function that takes a goal-scoring rate, {\tt lam}, in goals
per game, and simulates a game by generating the time between goals
until the total time exceeds 1 game, then returns the number of goals
scored.

Write another function that simulates many games, stores the
estimates of {\tt lam}, then computes their mean error and RMSE.

Is this way of making an estimate biased?  Plot the sampling
distribution of the estimates and the 90\% confidence interval.  What
is the standard error?  What happens to sampling error for increasing
values of {\tt lam}?
\index{estimator!biased}
\index{biased estimator}
\index{standard error}
\index{confidence interval}

\end{exercise}


\section{Glossary}

\begin{itemize}

\item estimation: The process of inferring the parameters of a distribution
from a sample.
\index{estimation}

\item estimator: A statistic used to estimate a parameter.
\index{estimation}

\item mean squared error (MSE): A measure of estimation error.
\index{mean squared error}
\index{MSE}

\item root mean squared error (RMSE): The square root of MSE,
a more meaningful representation of typical error magnitude.
\index{mean squared error}
\index{MSE}

\item maximum likelihood estimator (MLE): An estimator that computes the
point estimate most likely to be correct.
\index{MLE}
\index{maximum likelihood estimator}

\item bias (of an estimator): The tendency of an estimator to be above or
  below the actual value of the parameter, when averaged over repeated
  experiments.  \index{biased estimator}

\item sampling error: Error in an estimate due to the limited
  size of the sample and variation due to chance. \index{point estimation}

\item sampling bias: Error in an estimate due to a sampling process
  that is not representative of the population. \index{sampling bias}

\item measurement error: Error in an estimate due to inaccuracy collecting
  or recording data. \index{measurement error}

\item sampling distribution: The distribution of a statistic if an
  experiment is repeated many times.  \index{sampling distribution}

\item standard error: The RMSE of an estimate,
which quantifies variability due to sampling error (but not
other sources of error).
\index{standard error}

\item confidence interval: An interval that represents the expected
  range of an estimator if an experiment is repeated many times.
  \index{confidence interval} \index{interval!confidence}

\end{itemize}
