

\chapter{가설 검정 (Hypothesis testing)}
\label{testing}

이번 장에서 사용되는 코드는 {\tt hypothesis.py}에 있다.
코드를 다운로드하고 작업하는 것에 대한 정보는 ~\ref{code}을 참조한다.


\section{전통적 가설 검정 (Classical hypothesis testing)}
\index{가설 검정 (hypothesis testing)}
\index{명백한 효과 (apparent effect)}

NSFG에서 데이터를 탐색하면서, 첫째 아이와 첫째가 아닌 아이들 간 차이를 
포함해서 몇가지 ``명백한 효과 (apparent effects)''를 봤다.
지금까지 액면 그대로 이러한 효과를 받아들였다; 이번 장에서 이를 검정한다. 

\index{국가 가정 성장 조사 (National Survey of Family Growth)}
\index{NSFG}

다루고자 하는 근본적인 질문은 표본에서 본 효과가 더 큰 모집단에서 나타날 것인가다.
예를 들어, NSFG 표본에서 첫째 아이와 첫째 아이가 아닌 아이들에 대한 평균 임신 기간에
차이를 봤다. 알고자 하는 것은 이 효과가 미국 여성에 대한 진정한 차이를 반영하는지 
혹은, 우연히 표본에서 생겨난 것이냐다.
\index{임신 기간 (pregnancy length)} 
\index{기간 (length)!임신 (pregnancy)}

피셔 귀무 가설 검정(Fisher null hypothesis testing), 네이만 피어슨 의사결정 이론(Neyman-Pearson decision theory), 베이즈 추론(Bayesian inference)\footnote{베이즈 추론에 대한 좀더 많은 정보는 후속해서 출간되는 {\it Think Bayes}를 참조한다.}을 포함해서 질문을 구성하는 방법이 몇가지 있다.
여기 제시하는 것은 대부분의 사람이 실무에서 사용하는 세가지 방법의 일부분으로 {\bf 전통적 가설 검정 (classical hypothesis testing)}이라고 저자가 작명한 것이다.
\index{베이즈 추론 (Bayesian inference)}
\index{귀무 가설 (null hypothesis)}

전통적 가설 검정의 목적은 다음 질문에 답하는 것이다.
``표본과 명백한 효과가 주어졌다면, 우연히 그런 효과를 목도하는 확률이 얼마인가?''
다음에 질문에 답을 하는 방법이 있다.

\begin{itemize}

\item 첫번째 단계는 {\bf 검정 통계량 (test statistic)}을 선택해서 
명백한 효과 크기를 정량화한다. NSFG 예제에서, 명백한 효과는 첫번째 아이와 첫째가 아닌 아이들 사이에 임신 기간에 차이가 된다. 그래서, 검정 통계량에 대한 자연스러운 선택이
두 집단 사이에 평균 차이가 된다.
  \index{검정 통계량 (test statistic)}

\item 두번째 단계는 {\bf 귀무 가설 (null hypothesis)}을 정의하는 것으로,
명백한 효과가 사실이 아니라는 가정에 기반한 시스템 모형이다.
NSFG 예제에서, 귀무 가설은 첫째 아이와 첫째가 아닌 아이들 사이에 차이가 없다가 된다;
즉, 두 집단 임신 기간은 동일한 분포다. 
\index{귀무 가설 (null hypothesis)}
\index{임신 기간 (pregnancy length)}
\index{모형 (model)}

\item 세번째 단계는 {\bf p-값(p-value)}을 계산하는 것으로, 만약 귀무 가설이 사실이라면 명백한 효과를 볼 확률값이 된다. NSFG 예제에서, 평균에 실제 차이를 계산하고 나서, 귀무 가설 아래에서 차이를 크게 혹은 더 크게 볼 확률을 계산한다.
\index{p-값 (p-value)}

\item 마지막 단계는 결과를 해석하는 것이다. 만약 p-값이 낮다면, 효과에 
{\bf 통계적 유의성 (statistically significant)}이 있다고 한다.
우연히 발생할 것 같지 않다는 의미가 된다. 이 경우 더 큰 모집단에서 효과가 나타날 것 같다고 추론한다.\index{통계적 유의성 (statistically significant)} 
\index{유의성 (significant)}

\end{itemize}

이 과정의 로직(logic)은 모순(contradiction)에 의한 증명과 유사하다. 
수학 명제(mathematical statement) A를 증명하기 위해, 일시적으로 A 가 거짓이라고 가정한다.
만약 가정이 모순을 이끌게 되면, A 는 사실 참이어야 한다고 결론낸다.

\index{모순, 증명 (contradiction, proof by)}
\index{모순에 의한 증명 (proof by contradiction)}

유사하게, ``효과가 사실 (This effect is real)'' 같은 가설을 검정하기 위해서,
일시적으로 효과가 없다고 가정한다. 이것이 귀무가설이다.
이 가정에 기반해서, 명백한 효과 확률을 계산한다. 이것이 p-값이다.
만약 p-값이 작다면, 귀무 가설이 사실이 아닐 것 같다고 결론낸다.

\index{p-값 (p-value)}
\index{귀무 가설 (null hypothesis)}


\section{HypothesisTest}
\label{hypotest}
\index{평균 (mean)!차이 (difference in)}

{\tt thinkstats2}에는 전통적 가설 검정 구조를 표현하는 {\tt HypothesisTest} 클래스가 있다.

다음에 클래스 정의가 있다.
\index{HypothesisTest}

\begin{verbatim}
class HypothesisTest(object):

    def __init__(self, data):
        self.data = data
        self.MakeModel()
        self.actual = self.TestStatistic(data)

    def PValue(self, iters=1000):
        self.test_stats = [self.TestStatistic(self.RunModel()) 
                           for _ in range(iters)]

        count = sum(1 for x in self.test_stats if x >= self.actual)
        return count / iters

    def TestStatistic(self, data):
        raise UnimplementedMethodException()

    def MakeModel(self):
        pass

    def RunModel(self):
        raise UnimplementedMethodException()
\end{verbatim}

{\tt HypothesisTest}는 추상 부모 클래스로 메쏘드 몇개에 대한 완전한 정의와 
다른 메쏘드를 위한 자리잡기(place-keeper) 기능 제공한다.
{\tt HypothesisTest}에 기반한 자식 클래스는 \verb"__init__"와 {\tt PValue} 을 상속받고, {\tt TestStatistic}, {\tt RunModel}, 그리고 선택옵션으로 {\tt MakeModel} 메쏘드를 제공한다.
\index{HypothesisTest}

\verb"__init__"은 적절한 어떤 형식의 데이터도 받아들인다.
{\tt MakeModel} 호출해서 귀무 가설을 표현을 구축하고 나서,
데이터를 {\tt TestStatistic}에 전달하고 표본 효과 크기를 계산한다.

\index{검정 통계량 (test statistic)}
\index{귀무 가설 (null hypothesis)}

{\tt PValue}는 귀무 가설 아래에서 명백한 효과 확률을 계산한다.
매개 변수로 {\tt iters}을 받는데 실행할 모의 시험 횟수다.
첫번째 행은 모의 시험 데이터를 생성하고, 검정 통계량을 계산하고, 
\verb"test_stats"에 저장한다.
결과는 관측된 검정 통계량 {\tt self.actual}와 동일하거나 큰 \verb"test_stats" 에 있는 비율이다.
\index{모의 시험 (simulation)}

간단한 예제\footnote{인용 출처: MacKay, {\it Information
Theory, Inference, and Learning Algorithms}, 2003.}로, 250번 동전을 던져서 앞면 140회, 뒷면 110회 나왔다고 가정하자.
이 결과에 기초하여, 동전에 편의가 있는지 의심할지도 모른다; 즉, 좀더 앞면이 나올 것 같다.
이 가설을 검정하기 위해서, 확률을 계산해서 만약 동전이 정말 공정하다면, 그런 차이가 있는지 살펴본다.

\index{편의 동전 (biased coin)}
\index{MacKay, David}

\begin{verbatim}
class CoinTest(thinkstats2.HypothesisTest):

    def TestStatistic(self, data):
        heads, tails = data
        test_stat = abs(heads - tails)
        return test_stat

    def RunModel(self):
        heads, tails = self.data
        n = heads + tails
        sample = [random.choice('HT') for _ in range(n)]
        hist = thinkstats2.Hist(sample)
        data = hist['H'], hist['T']
        return data
\end{verbatim}

모수 {\tt data}는 정수 짝이다: 앞면 숫자, 뒷면 숫자.
검정 통계량은 둘 사이 절대값 차이다. 그래서 {\tt self.actual}은 30 이다.
\index{HypothesisTest}

{\tt RunModel}은 동전이 정말 공정하다고 가정하고 동전던지기를 모의 시험한다.
250번 동전던지기 표본을 생성하고, Hist를 사용해서 앞면과 뒷면 횟수를 계수하고 나서,
정수 짝을 반환한다.
\index{Hist}
\index{모형 (model)}

이제 해야할 일은 {\tt CoinTest} 인스턴스화 하고, {\tt PValue}를 호출한다.

\begin{verbatim}
    ct = CoinTest((140, 110))
    pvalue = ct.PValue()
\end{verbatim}

결과는 약 0.07 이 된다. 만약 동전이 공정하다면, 30 만큼 차이는 이번에 약 7\% 확률로 기대할 수 있다.

이러한 결과를 어떻게 해석해야 할까요? 통상 관례로, 5\%가 통계적 유의성의 임계치다.
만약 p-값이 5\%보다 적다면, 효과가 유의적이라고 생각하고; 그렇지 않다면, 반대로 생각한다.

\index{p-값 (p-value)}
\index{통계적 유의성 (statistically significant)} 
\index{유의성 (significant)}

하지만, 5\% 선택은 작위적이고, (나중에 살펴보겠지만) p-값은 검정 통계량과 귀무 가설 모형에 의존성이 있다.
그래서, p-값이 정밀한 측정으로 생각하지는 말아야 한다.
\index{귀무 가설 (null hypothesis)}

p-값을 해석함에 있어 저자가 추전하는 것은 p-값 크기에 따라 달리하는 것이다:
만약 p-값이 1\%보다 작다면 효과가 우연에 의한 것일 것 같지는 않다;
만약 p-값이 10\%보다 크다면, 효과는 우연으로 설명될 수 있을 것 같다.
p-값이 1\%과 10\% 사이라면, 경계값으로 생각해야한다. 그래서, 상기 사례를 통해서는 
동전에 편의가 있는지 없는지 데이터가 강력한 증거를 주지는 못한다고 결론낸다. 

\section{평균 차이 검정 (Testing a difference in means)}
\label{testdiff}
\index{평균 (mean)!차이 (difference in)}

One of the most common effects to test is a difference in mean
between two groups.  In the NSFG data, we saw that the mean pregnancy
length for first babies is slightly longer, and the mean birth weight
is slightly smaller.  Now we will see if those effects are
statistically significant.
\index{National Survey of Family Growth}
\index{NSFG}
\index{pregnancy length}
\index{length!pregnancy}

For these examples, the null hypothesis is that the distributions
for the two groups are the same.  One way to model the null
hypothesis is by {\bf permutation}; that is, we can take values
for first babies and others and shuffle them, treating
the two groups as one big group:
\index{null hypothesis}
\index{permutation}
\index{model}

\begin{verbatim}
class DiffMeansPermute(thinkstats2.HypothesisTest):

    def TestStatistic(self, data):
        group1, group2 = data
        test_stat = abs(group1.mean() - group2.mean())
        return test_stat

    def MakeModel(self):
        group1, group2 = self.data
        self.n, self.m = len(group1), len(group2)
        self.pool = np.hstack((group1, group2))

    def RunModel(self):
        np.random.shuffle(self.pool)
        data = self.pool[:self.n], self.pool[self.n:]
        return data
\end{verbatim}

{\tt data} is a pair of sequences, one for each
group.  The test statistic is the absolute difference in the means.
\index{HypothesisTest}

{\tt MakeModel} records the sizes of the groups, {\tt n} and
{\tt m}, and combines the groups into one NumPy
array, {\tt self.pool}.
\index{NumPy}

{\tt RunModel} simulates the null hypothesis by shuffling the
pooled values and splitting them into two groups with sizes {\tt n}
and {\tt m}.  As always, the return value from {\tt RunModel} has
the same format as the observed data.
\index{null hypothesis}
\index{model}

To test the difference in pregnancy length, we run:

\begin{verbatim}
    live, firsts, others = first.MakeFrames()
    data = firsts.prglngth.values, others.prglngth.values
    ht = DiffMeansPermute(data)
    pvalue = ht.PValue()
\end{verbatim}

{\tt MakeFrames} reads the NSFG data and returns DataFrames
representing all live births, first babies, and others.
We extract pregnancy lengths as NumPy arrays, pass them as
data to {\tt DiffMeansPermute}, and compute the p-value.  The
result is about 0.17, which means that we expect to see a difference
as big as the observed effect about 17\% of the time.  So
this effect is not statistically significant.
\index{DataFrame}
\index{p-value}
  \index{significant} \index{statistically significant}
\index{pregnancy length}

\begin{figure}
% hypothesis.py
\centerline{\includegraphics[height=2.5in]{figs/hypothesis1.pdf}}
\caption{CDF of difference in mean pregnancy length under the null
hypothesis.}
\label{hypothesis1}
\end{figure}

{\tt HypothesisTest} provides {\tt PlotCdf}, which plots the
distribution of the test statistic and a gray line indicating
the observed effect size:
\index{thinkplot}
\index{HypothesisTest}
\index{Cdf}
\index{effect size}

\begin{verbatim}
    ht.PlotCdf()
    thinkplot.Show(xlabel='test statistic',
                   ylabel='CDF')
\end{verbatim}

Figure~\ref{hypothesis1} shows the result.  The CDF intersects the
observed difference at 0.83, which is the complement of the p-value,
0.17.
\index{p-value}

If we run the same analysis with birth weight, the computed p-value
is 0; after 1000 attempts,
the simulation never yields an effect
as big as the observed difference, 0.12 lbs.  So we would
report $p < 0.001$, and
conclude that the difference in birth weight is statistically
significant.
\index{birth weight}
\index{weight!birth}
  \index{significant} \index{statistically significant}


\section{Other test statistics}

Choosing the best test statistic depends on what question you are
trying to address.  For example, if the relevant question is whether
pregnancy lengths are different for first
babies, then it makes sense to test the absolute difference in means,
as we did in the previous section.
\index{test statistic}
\index{pregnancy length}

If we had some reason to think that first babies are likely
to be late, then we would not take the absolute value of the difference;
instead we would use this test statistic:

\begin{verbatim}
class DiffMeansOneSided(DiffMeansPermute):

    def TestStatistic(self, data):
        group1, group2 = data
        test_stat = group1.mean() - group2.mean()
        return test_stat
\end{verbatim}

{\tt DiffMeansOneSided} inherits {\tt MakeModel} and {\tt RunModel}
from {\tt DiffMeansPermute}; the only difference is that
{\tt TestStatistic} does not take the absolute value of the
difference.  This kind of test is called {\bf one-sided} because
it only counts one side of the distribution of differences.  The
previous test, using both sides, is {\bf two-sided}.
\index{one-sided test}
\index{two-sided test}

For this version of the test, the p-value is 0.09.  In general
the p-value for a one-sided test is about half the p-value for
a two-sided test, depending on the shape of the distribution.
\index{p-value}

The one-sided hypothesis, that first babies are born late, is more
specific than the two-sided hypothesis, so the p-value is smaller.
But even for the stronger hypothesis, the difference is
not statistically significant.
  \index{significant} \index{statistically significant}

We can use the same framework to test for a difference in standard
deviation.  In Section~\ref{visualization}, we saw some evidence that
first babies are more likely to be early or late, and less likely to
be on time.  So we might hypothesize that the standard deviation is
higher.  Here's how we can test that:
\index{standard deviation}

\begin{verbatim}
class DiffStdPermute(DiffMeansPermute):

    def TestStatistic(self, data):
        group1, group2 = data
        test_stat = group1.std() - group2.std()
        return test_stat
\end{verbatim}

This is a one-sided test because the hypothesis is that the standard
deviation for first babies is higher, not just different.  The p-value
is 0.09, which is not statistically significant.
\index{p-value}
\index{permutation}
  \index{significant} \index{statistically significant}


\section{Testing a correlation}
\label{corrtest}

This framework can also test correlations.  For example, in the NSFG
data set, the correlation between birth weight and mother's age is
about 0.07.  It seems like older mothers have heavier babies.  But
could this effect be due to chance?
\index{correlation}
\index{test statistic}

For the test statistic, I use
Pearson's correlation, but Spearman's would work as well.
If we had reason to expect positive correlation, we would do a
one-sided test.  But since we have no such reason, I'll
do a two-sided test using the absolute value of correlation.
\index{Pearson coefficient of correlation}
\index{Spearman coefficient of correlation}

The null hypothesis is that there is no correlation between mother's
age and birth weight.  By shuffling the observed values, we can
simulate a world where the distributions of age and
birth weight are the same, but where the variables are unrelated:
\index{birth weight}
\index{weight!birth}
\index{null hypothesis}

\begin{verbatim}
class CorrelationPermute(thinkstats2.HypothesisTest):

    def TestStatistic(self, data):
        xs, ys = data
        test_stat = abs(thinkstats2.Corr(xs, ys))
        return test_stat

    def RunModel(self):
        xs, ys = self.data
        xs = np.random.permutation(xs)
        return xs, ys
\end{verbatim}

{\tt data} is a pair of sequences.  {\tt TestStatistic} computes the
absolute value of Pearson's correlation.  {\tt RunModel} shuffles the
{\tt xs} and returns simulated data.
\index{HypothesisTest}
\index{permutation}
\index{Pearson coefficient of correlation}

Here's the code that reads the data and runs the test:

\begin{verbatim}
    live, firsts, others = first.MakeFrames()
    live = live.dropna(subset=['agepreg', 'totalwgt_lb'])
    data = live.agepreg.values, live.totalwgt_lb.values
    ht = CorrelationPermute(data)
    pvalue = ht.PValue()
\end{verbatim}

I use {\tt dropna} with the {\tt subset} argument to drop rows
that are missing either of the variables we need.
\index{dropna}
\index{NaN}
\index{missing values}

The actual correlation is 0.07.  The computed p-value is 0; after 1000
iterations the largest simulated correlation is 0.04.  So although the
observed correlation is small, it is statistically significant.
\index{p-value}
  \index{significant} \index{statistically significant}

This example is a reminder that ``statistically significant'' does not
always mean that an effect is important, or significant in practice.
It only means that it is unlikely to have occurred by chance.


\section{Testing proportions}
\label{casino}
\index{chi-squared test}

Suppose you run a casino and you suspect that a customer is
using a crooked die; that
is, one that has been modified to make one of the faces more
likely than the others.  You apprehend the alleged
cheater and confiscate the die, but now you have to prove that it
is crooked.  You roll the die 60 times and get the following results:
\index{casino}
\index{dice}
\index{crooked die}

\begin{center}
\begin{tabular}{|l|c|c|c|c|c|c|}
\hline
Value     &  1  &  2  &  3  &  4  &  5  &  6  \\ 
\hline
Frequency &  8  &  9  &  19  &  5  &  8  &  11  \\
\hline
\end{tabular}
\end{center}

On average you expect each value to appear 10 times.  In this
dataset, the value 3 appears more often than expected, and the value 4
appears less often.  But are these differences statistically
significant?
\index{frequency}
  \index{significant} \index{statistically significant}

To test this hypothesis, we can compute the expected frequency for
each value, the difference between the expected and observed
frequencies, and the total absolute difference.  In this
example, we expect each side to come up 10 times out of 60; the
deviations from this expectation are -2, -1, 9, -5, -2, and 1; so the
total absolute difference is 20.  How often would we see such a
difference by chance?
\index{deviation}

Here's a version of {\tt HypothesisTest} that answers that question:
\index{HypothesisTest}

\begin{verbatim}
class DiceTest(thinkstats2.HypothesisTest):

    def TestStatistic(self, data):
        observed = data
        n = sum(observed)
        expected = np.ones(6) * n / 6
        test_stat = sum(abs(observed - expected))
        return test_stat

    def RunModel(self):
        n = sum(self.data)
        values = [1, 2, 3, 4, 5, 6]
        rolls = np.random.choice(values, n, replace=True)
        hist = thinkstats2.Hist(rolls)
        freqs = hist.Freqs(values)
        return freqs
\end{verbatim}

The data are represented as a list of frequencies: the observed
values are {\tt [8, 9, 19, 5, 8, 11]}; the expected frequencies
are all 10.  The test statistic is the sum of the absolute differences.
\index{frequency}

The null hypothesis is that the die is fair, so we simulate that by
drawing random samples from {\tt values}.  {\tt RunModel} uses {\tt
  Hist} to compute and return the list of frequencies.
\index{Hist}
\index{null hypothesis}
\index{model}

The p-value for this data is 0.13, which means that if the die is
fair we expect to see the observed total deviation, or more, about
13\% of the time.  So the apparent effect is not statistically
significant.
\index{p-value}
\index{deviation}
  \index{significant} \index{statistically significant}


\section{Chi-squared tests}
\label{casino2}

In the previous section we used total deviation as the test statistic.
But for testing proportions it is more common to use the chi-squared
statistic:
%
\[ \goodchi^2 = \sum_i \frac{(O_i - E_i)^2}{E_i} \]
%
%% TODO: Consider using upper case chi, which is more strictly correct,
%% but harder to distinguish from X.
% 
Where $O_i$ are the observed frequencies and $E_i$ are the expected
frequencies.  Here's the Python code:
\index{chi-squared test}
\index{chi-squared statistic}
\index{test statistic}

\begin{verbatim}
class DiceChiTest(DiceTest):

    def TestStatistic(self, data):
        observed = data
        n = sum(observed)
        expected = np.ones(6) * n / 6
        test_stat = sum((observed - expected)**2 / expected)
        return test_stat
\end{verbatim}

Squaring the deviations (rather than taking absolute values) gives
more weight to large deviations.  Dividing through by {\tt expected}
standardizes the deviations, although in this case it has no effect
because the expected frequencies are all equal.
\index{deviation}

The p-value using the chi-squared statistic is 0.04,
substantially smaller than what we got using total deviation, 0.13.
If we take the 5\% threshold seriously, we would consider this effect
statistically significant.  But considering the two tests togther, I
would say that the results are borderline.  I would not rule out the
possibility that the die is crooked, but I would not convict the
accused cheater.
\index{p-value}
  \index{significant} \index{statistically significant}

This example demonstrates an important point: the p-value depends
on the choice of test statistic and the model of the null hypothesis,
and sometimes these choices determine whether an effect is
statistically significant or not.
\index{null hypothesis}
\index{model}


\section{First babies again}

Earlier in this chapter we looked at pregnancy lengths for first
babies and others, and concluded that the apparent differences in
mean and standard deviation are not statistically significant.  But in
Section~\ref{visualization}, we saw several apparent differences
in the distribution of pregnancy length, especially in the range from
35 to 43 weeks.  To see whether those differences are statistically
significant, we can use a test based on a chi-squared statistic.
\index{standard deviation}
\index{statistically significant} \index{significant}
\index{pregnancy length}

The code combines elements from previous examples:
\index{HypothesisTest}

\begin{verbatim}
class PregLengthTest(thinkstats2.HypothesisTest):

    def MakeModel(self):
        firsts, others = self.data
        self.n = len(firsts)
        self.pool = np.hstack((firsts, others))

        pmf = thinkstats2.Pmf(self.pool)
        self.values = range(35, 44)
        self.expected_probs = np.array(pmf.Probs(self.values))

    def RunModel(self):
        np.random.shuffle(self.pool)
        data = self.pool[:self.n], self.pool[self.n:]
        return data
\end{verbatim}

The data are represented as two lists of pregnancy lengths.  The null
hypothesis is that both samples are drawn from the same distribution.
{\tt MakeModel} models that distribution by pooling the two
samples using {\tt hstack}.  Then {\tt RunModel} generates
simulated data by shuffling the pooled sample and splitting it
into two parts.
\index{null hypothesis}
\index{model}
\index{hstack}
\index{pregnancy length}

{\tt MakeModel} also defines {\tt values}, which is the
range of weeks we'll use, and \verb"expected_probs",
which is the probability of each value in the pooled distribution.

Here's the code that computes the test statistic:

\begin{verbatim}
# class PregLengthTest:

    def TestStatistic(self, data):
        firsts, others = data
        stat = self.ChiSquared(firsts) + self.ChiSquared(others)
        return stat

    def ChiSquared(self, lengths):
        hist = thinkstats2.Hist(lengths)
        observed = np.array(hist.Freqs(self.values))
        expected = self.expected_probs * len(lengths)
        stat = sum((observed - expected)**2 / expected)
        return stat
\end{verbatim}

{\tt TestStatistic} computes the chi-squared statistic for
first babies and others, and adds them.
\index{chi-squared statistic}

{\tt ChiSquared} takes a sequence of pregnancy lengths, computes
its histogram, and computes {\tt observed}, which is a list of
frequencies corresponding to {\tt self.values}.
To compute the list of expected frequencies, it multiplies the
pre-computed probabilities, \verb"expected_probs", by the sample
size.  It returns the chi-squared statistic, {\tt stat}.

For the NSFG data the total chi-squared statistic is 102, which
doesn't mean much by itself.  But after 1000 iterations, the largest
test statistic generated under the null hypothesis is 32.  We conclude
that the observed chi-squared statistic is unlikely under the null
hypothesis, so the apparent effect is statistically significant.
\index{null hypothesis}
\index{statistically significant} \index{significant}

This example demonstrates a limitation of chi-squared tests: they
indicate that there is a difference between the two groups,
but they don't say anything specific about what the difference is.


\section{Errors}
\index{error}

In classical hypothesis testing, an effect is considered statistically
significant if the p-value is below some threshold, commonly 5\%.
This procedure raises two questions:
\index{p-value}
\index{threshold}
\index{statistically significant} \index{significant}

\begin{itemize}

\item If the effect is actually due to chance, what is the probability
that we will wrongly consider it significant?  This
probability is the {\bf false positive rate}.
\index{false positive}

\item If the effect is real, what is the chance that the hypothesis
test will fail?  This probability is the {\bf false negative rate}.
\index{false negative}

\end{itemize}

The false positive rate is relatively easy to compute: if the
threshold is 5\%, the false positive rate is 5\%.  Here's why:

\begin{itemize}

\item If there is no real effect, the null hypothesis is true, so we
  can compute the distribution of the test statistic by simulating the
  null hypothesis.  Call this distribution $\CDF_T$.
\index{null hypothesis}
\index{CDF}

\item Each time we run an experiment, we get a test statistic, $t$,
  which is drawn from $CDF_T$.  Then we compute a p-value, which is
  the probability that a random value from $CDF_T$ exceeds {\tt t},
  so that's $1 - CDF_T(t)$.

\item The p-value is less than 5\% if $CDF_T(t)$ is greater
  than 95\%; that is, if $t$ exceeds the 95th percentile.
  And how often does a value chosen from $CDF_T$ exceed
  the 95th percentile?  5\% of the time.

\end{itemize}

So if you perform one hypothesis test with a 5\% threshold, you expect
a false positive 1 time in 20.


\section{Power}
\label{power}

The false negative rate is harder to compute because it depends on
the actual effect size, and normally we don't know that.
One option is to compute a rate
conditioned on a hypothetical effect size.
\index{effect size}

For example, if we assume that the observed difference between groups
is accurate, we can use the observed samples as a model of the
population and run hypothesis tests with simulated data:
\index{model}

\begin{verbatim}
def FalseNegRate(data, num_runs=100):
    group1, group2 = data
    count = 0

    for i in range(num_runs):
        sample1 = thinkstats2.Resample(group1)
        sample2 = thinkstats2.Resample(group2)

        ht = DiffMeansPermute((sample1, sample2))
        pvalue = ht.PValue(iters=101)
        if pvalue > 0.05:
            count += 1

    return count / num_runs
\end{verbatim}

{\tt FalseNegRate} takes data in the form of two sequences, one for
each group.  Each time through the loop, it simulates an experiment by
drawing a random sample from each group and running a hypothesis test.
Then it checks the result and counts the number of false negatives.
\index{Resample}
\index{permutation}

{\tt Resample} takes a sequence and draws a sample with the same
length, with replacement:
\index{replacement}

\begin{verbatim}
def Resample(xs):
    return np.random.choice(xs, len(xs), replace=True)
\end{verbatim}

Here's the code that tests pregnancy lengths:

\begin{verbatim}
    live, firsts, others = first.MakeFrames()
    data = firsts.prglngth.values, others.prglngth.values
    neg_rate = FalseNegRate(data)
\end{verbatim}

The result is about 70\%, which means that if the actual difference in
mean pregnancy length is 0.078 weeks, we expect an experiment with this
sample size to yield a negative test 70\% of the time.
\index{pregnancy length}

This result is often presented the other way around: if the actual
difference is 0.078 weeks, we should expect a positive test only 30\%
of the time.  This ``correct positive rate'' is called the {\bf power}
of the test, or sometimes ``sensitivity''.  It reflects the ability of
the test to detect an effect of a given size.
\index{power}
\index{sensitivity}
\index{correct positive}

In this example, the test had only a 30\% chance of yielding a
positive result (again, assuming that the difference is 0.078 weeks).
As a rule of thumb, a power of 80\% is considered acceptable, so
we would say that this test was ``underpowered.''
\index{underpowered}

In general a negative hypothesis test does not imply that there is no
difference between the groups; instead it suggests that if there is a
difference, it is too small to detect with this sample size.


\section{Replication}
\label{replication}

The hypothesis testing process I demonstrated in this chapter is not,
strictly speaking, good practice.

First, I performed multiple tests.  If you run one hypothesis test,
the chance of a false positive is about 1 in 20, which might be
acceptable.  But if you run 20 tests, you should expect at least one
false positive, most of the time.
\index{multiple tests}

Second, I used the same dataset for exploration and testing.  If
you explore a large dataset, find a surprising effect, and then test
whether it is significant, you have a good chance of generating a
false positive.
\index{statistically significant} \index{significant}

To compensate for multiple tests, you can adjust the p-value
threshold (see
  \url{https://en.wikipedia.org/wiki/Holm-Bonferroni_method}).  Or you
can address both problems by partitioning the data, using one set for
exploration and the other for testing.
\index{p-value}
\index{Holm-Bonferroni method}

In some fields these practices are required or at least encouraged.
But it is also common to address these problems implicitly by
replicating published results.  Typically the first paper to report a
new result is considered exploratory.  Subsequent papers that
replicate the result with new data are considered confirmatory.
\index{confirmatory result}

As it happens, we have an opportunity to replicate the results in this
chapter.  The first edition of this book is based on Cycle 6 of the
NSFG, which was released in 2002.  In October 2011, the CDC released
additional data based on interviews conducted from 2006--2010.  {\tt
  nsfg2.py} contains code to read and clean this data.  In the new
dataset:
\index{NSFG}

\begin{itemize}

\item The difference in mean pregnancy length is
0.16 weeks and statistically significant with $p < 0.001$ (compared
to 0.078 weeks in the original dataset).
\index{statistically significant} \index{significant}
\index{pregnancy length}

\item The difference in birth weight is 0.17 pounds with $p < 0.001$
(compared to 0.12 lbs in the original dataset).
\index{birth weight}
\index{weight!birth}

\item The correlation between birth weight and mother's age is
0.08 with $p < 0.001$ (compared to 0.07).

\item The chi-squared test is statistically significant with
$p < 0.001$ (as it was in the original).

\end{itemize}

In summary, all of the effects that were statistically significant
in the original dataset were replicated in the new dataset, and the
difference in pregnancy length, which was not significant in the
original, is bigger in the new dataset and significant.


\section{Exercises}

A solution to these exercises is in \verb"chap09soln.py".

\begin{exercise}
As sample size increases, the power of a hypothesis test increases,
which means it is more likely to be positive if the effect is real.
Conversely, as sample size decreases, the test is less likely to
be positive even if the effect is real.
\index{sample size}

To investigate this behavior, run the tests in this chapter with
different subsets of the NSFG data.  You can use {\tt thinkstats2.SampleRows}
to select a random subset of the rows in a DataFrame.
\index{National Survey of Family Growth}
\index{NSFG}
\index{DataFrame}

What happens to the p-values of these tests as sample size decreases?
What is the smallest sample size that yields a positive test?
\index{p-value}
\end{exercise}



\begin{exercise}

In Section~\ref{testdiff}, we simulated the null hypothesis by
permutation; that is, we treated the observed values as if they
represented the entire population, and randomly assigned the
members of the population to the two groups.
\index{null hypothesis}
\index{permutation}

An alternative is to use the sample to estimate the distribution for
the population, then draw a random sample from that distribution.
This process is called {\bf resampling}.  There are several ways to
implement resampling, but one of the simplest is to draw a sample
with replacement from the observed values, as in Section~\ref{power}.
\index{resampling}
\index{replacement}

Write a class named {\tt DiffMeansResample} that inherits from
{\tt DiffMeansPermute} and overrides {\tt RunModel} to implement
resampling, rather than permutation.
\index{permutation}

Use this model to test the differences in pregnancy length and
birth weight.  How much does the model affect the results?
\index{model}
\index{birth weight}
\index{weight!birth}
\index{pregnancy length}

\end{exercise}


\section{Glossary}

\begin{itemize}

\item hypothesis testing: The process of determining whether an apparent
effect is statistically significant.
\index{hypothesis testing}

\item test statistic: A statistic used to quantify an effect size.
\index{test statistic}
\index{effect size}

\item null hypothesis: A model of a system based on the assumption that
an apparent effect is due to chance.
\index{null hypothesis}

\item p-value: The probability that an effect could occur by chance.
\index{p-value}

\item statistically significant: An effect is statistically
  significant if it is unlikely to occur by chance.
  \index{significant} \index{statistically significant}

\item permutation test: A way to compute p-values by generating
  permutations of an observed dataset.
  \index{permutation test}

\item resampling test: A way to compute p-values by generating
  samples, with replacement, from an observed dataset.
  \index{resampling test}

\item two-sided test: A test that asks, ``What is the chance of an effect
as big as the observed effect, positive or negative?''

\item one-sided test: A test that asks, ``What is the chance of an effect
as big as the observed effect, and with the same sign?''
\index{one-sided test}
\index{two-sided test}
\index{test!one-sided}
\index{test!two-sided}

\item chi-squared test: A test that uses the chi-squared statistic as
the test statistic.
\index{chi-squared test}

\item false positive: The conclusion that an effect is real when it is not.
\index{false positive}

\item false negative: The conclusion that an effect is due to chance when it
is not.
\index{false negative}

\item power: The probability of a positive test if the null hypothesis
is false.
\index{power}
\index{null hypothesis}

\end{itemize}

