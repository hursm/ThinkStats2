
\chapter{선형최소제곱 (Linear least squares)}
\label{linear}

이번 장에서 사용되는 코드는 {\tt linear.py}에 있다.
코드를 다운로드하고 작업하는 것에 대한 정보는 ~\ref{code}을 참조한다.

\section{최소제곱 적합 (Least squares fit)}

상관계수는 관계 부호와 강도를 측정하지만 기울기는 측정하지 않는다.
기울기를 측정하는 방법이 몇가지 있다; 가장 흔한 방법이 {\bf 선형최소제곱 적합 (linear least squares fit)}이다. ``선형 적합(linear fit)''은 변수 사이 관계를 모형화하는 선(line)이다.
``최소제곱 (least squares)'' 적합은 선과 데이터 사이 평균제곱오차(mean
squared error, MSE)를 최소화하는 것이다.
\index{최소제곱 적합 (least squares fit)}
\index{선형최소제곱 (linear least squares)}
\index{모형 (model)}

하나 시퀀스 {\tt ys}가 있는데, 또 다른 시퀀스 {\tt xs} 함수로 표현하고자 한다고 가정하자.
만약 {\tt xs}, {\tt ys}와 절편 {\tt inter}, 기울기 {\tt slope} 사이에 선형 관계가 있다면,
각 {\tt y[i]} 가 {\tt inter + slope * x[i]}이 될 것으로 예상된다.  
\index{잔차 (residuals)}

하지만, 상관이 완벽하지 않다면, 예측은 단지 근사(approximation)가 된다.
선에서 수직 편차(vertical deviation), 즉 {\bf 잔차(residual)}는 다음과 같다. 
\index{편차 (deviation)}

\begin{verbatim}
res = ys - (inter + slope * xs)
\end{verbatim}

잔차는 측정 오차 같은 확률 요소(random factor), 혹은 알지 못하는 비임의 요소(non-random factor) 때문일지 모른다. 예를 들어, 만약 체중을 신장 함수로 예측한다면, 미지 요소는 식습관, 운동, 신체 유형을 포함할 수 있다.

\index{기울기 (slope)}
\index{절편 (intercept)}
\index{측정 오차 (measurement error)}

만약 모수 {\tt inter}와 {\tt slope}이 잘못되면, 잔차는 더 커진다.
그래서 모수는 잔차를 최소화한다는 것이 직관적으로 의미가 있다.
\index{모수 (parameter)}

잔차 절대값, 잔차 제곱, 혹은 잔차 세제곱 최소화를 시도해볼만 하다;
하지만, 가장 흔한 선택은 제곱 잔차 합을 최소화하는 것이다. {\tt sum(res**2))}.

왜 그럴까요? 세가지 좋은 이유와 한가지 덜 중요한 이유가 있다.

\begin{itemize}

\item 제곱하게 되면 양수 잔차와 음수 잔차를 동일하게 처리하는 기능이 있는데, 보통 원하는 것이다.

\item 제곱은 큰 잔차에 더 많은 가중치를 주지만, 가장 큰 잔차가 항상 주도적인 경우에는 그렇게 많은 가중치를 주지는 않는다.

\item 만약 잔차가 상관관계가 없고 평균과 상수 (하지만 알려지지 않은 미지) 분산을 가진 정규분포라면,
최소제곱 적합은 또한 {\tt inter}와 {\tt slope}의 최대우도추정량이다. \url{https://en.wikipedia.org/wiki/Linear_regression}.  
\index{MLE}
  \index{최대우도추정량 (maximum likelihood estimator)}
\index{상관 (correlation)}

\item 제곱 잔차를 최소화하는 {\tt inter}와 {\tt slope} 값은 효과적으로 계산될 수 있다.

\end{itemize}

계산 효율성(computational efficiency)이 당면한 문제에 가장 적합한 방법을 선택하는 것보다 더 중요할 때 마지막 이유가 의미있다. 
이제는 더 이상 그럴지는 않다. 그래서, 제곱 잔차가 최소화하는 올바른 것인지만 고려한다.
\index{계산 방법 (computational methods)}
\index{제곱잔차 (squared residuals)}

예를 들어, {\tt xs}을 사용해서 {\tt ys} 값을 예측하려고 한다면,
과다 추정하는 것이 과소 추정하는 것보다 더 좋을 수도 (더 나쁠 수도) 있다.
이런 경우, 각 잔차에 대한 비용함수를 계산하고 전체 비용, {\tt sum(cost(res))}을 최소화한다.
하지만, 최소제곱 적합을 계산하는 것이 빠르고, 쉽고, 종종 충분히 만족스럽다.
\index{비용 함수 (cost function)}


\section{구현 (Implementation)}

{\tt thinkstats2}에 선형최소제곱을 시연하는 간단한 함수가 있다.
\index{LeastSquares}

\begin{verbatim}
def LeastSquares(xs, ys):
    meanx, varx = MeanVar(xs)
    meany = Mean(ys)

    slope = Cov(xs, ys, meanx, meany) / varx
    inter = meany - slope * meanx

    return inter, slope
\end{verbatim}

{\tt LeastSquares}는 시퀀스 {\tt xs}와 {\tt ys}을 인자로 받고 추정한 모수 {\tt inter}와
{\tt slope}을 반환한다. 동작방법에 관한 자세한 사항은 웹사이트 참조. \url{http://wikipedia.org/wiki/Numerical_methods_for_linear_least_squares}.
\index{모수 (parameter)}


{\tt thinkstats2}는 {\tt FitLine}를 제공하는데, {\tt inter} 와 {\tt slope}을 인자로 받아서 시퀀스 {\tt xs}에 대해서 적합선을 반환한다.
\index{FitLine}

\begin{verbatim}
def FitLine(xs, inter, slope):
    fit_xs = np.sort(xs)
    fit_ys = inter + slope * fit_xs
    return fit_xs, fit_ys
\end{verbatim}

이 함수를 사용해서 산모 연령 함수로 출생 체중에 대한 최소제곱을 계산할 수 있다.
\index{출생 체중 (birth weight)}
\index{체중 (weight)!출생 (birth)}
\index{연령 (age)}

\begin{verbatim}
    live, firsts, others = first.MakeFrames()
    live = live.dropna(subset=['agepreg', 'totalwgt_lb'])
    ages = live.agepreg
    weights = live.totalwgt_lb

    inter, slope = thinkstats2.LeastSquares(ages, weights)
    fit_xs, fit_ys = thinkstats2.FitLine(ages, inter, slope)
\end{verbatim}

추정한 절편과 기울기는 년마다 6.8 lbs, 0.017 lbs 이다.
이러 형태로 값을 해석하기는 어렵다: 절편은 산모 연령이 0 에서 신생아 기대 체중인데,
문맥상 의미가 없고, 기울기는 너무 작아서 쉽게 이해가 되지 않는다.
\index{기울기 (slope)}
\index{절편 (intercept)}
\index{dropna}
\index{NaN}


$x=0$에 절편을 제시하는 대신에, 절편을 평균에 제시하는 것이 종종 도움이 된다.
이 경우에, 평균 나이는 약 25세이고, 25세 산모에 대한 평균 아이 체중은 7.3 파운드다.
기울기는 년마다 0.27 온스(ounces) 즉, 10년마다 0.17 파운드가 된다.

\begin{figure}
% linear.py
\centerline{\includegraphics[height=2.5in]{figs/linear1.pdf}}
\caption{Scatter plot of birth weight and mother's age with
a linear fit.}
\label{linear1}
\end{figure}

그림~\ref{linear1}에 적합선과 함께 출생 체중과 연령을 산점도로 보여준다.
이와 같이, 관계가 선형인지, 적합선이 관계를 나타내는 좋은 모형인지를 평가하는데, 그림을 그려보는 것은 좋은 생각이 된다. 

\index{출생 체중 (birth weight)}
\index{체중 (weight)!출생 (birth)}
\index{산점도 (scatter plot)}
\index{그림 (plot)!산점 (scatter)}
\index{모형 (model)}


\section{잔차 (Residuals)}
\label{residuals}

또다른 유용한 검정은 잔차를 플롯하여 그리는 것이다.
{\tt thinkstats2}에는 잔차를 계산하는 함수가 있다.
\index{잔차 (residuals)}

\begin{verbatim}
def Residuals(xs, ys, inter, slope):
    xs = np.asarray(xs)
    ys = np.asarray(ys)
    res = ys - (inter + slope * xs)
    return res
\end{verbatim}

{\tt Residuals}는 시퀀스 {\tt xs}와 {\tt ys}, 추정한 {\tt inter}와 {\tt slope}를 인자로 받는다. 실제값과 적합선 사이 차이를 반환한다.

\begin{figure}
% linear.py
\centerline{\includegraphics[height=2.5in]{figs/linear2.pdf}}
\caption{Residuals of the linear fit.}
\label{linear2}
\end{figure}

잔차를 시각화하기 위해서, ~\ref{characterizing} 절에서 살펴본 것과 같이, 응답자를 연령으로 묶고, 각 집단에 백분위수를 계산한다. 
그림~\ref{linear2}에 각 연령 집단에 대한 25번째, 50번째, 75번째 백분위수가 있다.
중위수는 예상한 것처럼 거의 0이고, 사분위수 범위는 약 2 파운드다. 그래서, 만약 산모 연령을 알고 있다면, 1 파운드 내에서 대략 50\% 가능성으로 아이 체중을 추측할 수 있다.
\index{시각화 (visualization)}

이상적으로 이들 직선이 평평해서 잔차가 랜덤(random)임을 나타내고, 
평행해서 잔차 분산이 모든 연령 집단에 대해서 동일하다는 것을 나타내야 한다.
사실, 직선은 평행에 가깝워서 좋다; 하지만, 약간의 곡률(curvature)이 있어서 관계가 비선형임을 나타낸다.
그럼에도 불구하고, 선형 적합은 간단한 모형으로 특정 목적에 아마도 잘 부합한다.  

\index{모형 (model)}
\index{비선형 (nonlinear)}


\section{추정 (Estimation)}
\label{regest}

모수 {\tt slope}와 {\tt inter}는 표본에 기반한 추정값이다; 다른 추정값처럼, 표집 편의, 측정 오차, 표집 오차에 휘둘리기 쉽다. 
~\ref{estimation} 장에서 논의한 것처럼, 표집 편의는 비대표 표집(non-representative sampling)에 의해서, 측정 오차는 데이터 수집과 기록 오류에 의해서, 표집 오차는 전체 모집단보다 표본을 측정하는 결과로 발생한다.
\index{표집 편의 (sampling bias)}
\index{편의 (bias)!표집 (sampling)}
\index{측정 오차 (measurement error)}
\index{표집 오차 (sampling error)}
\index{추정 (estimation)}

To assess sampling error, we ask, ``If we run this experiment again,
how much variability do we expect in the estimates?''  We can
answer this question by running simulated experiments and computing
sampling distributions of the estimates.
\index{sampling error}
\index{sampling distribution}

I simulate the experiments by resampling the data; that is, I treat
the observed pregnancies as if they were the entire population
and draw samples, with replacement, from the observed sample.
\index{simulation}
\index{replacement}

\begin{verbatim}
def SamplingDistributions(live, iters=101):
    t = []
    for _ in range(iters):
        sample = thinkstats2.ResampleRows(live)
        ages = sample.agepreg
        weights = sample.totalwgt_lb
        estimates = thinkstats2.LeastSquares(ages, weights)
        t.append(estimates)

    inters, slopes = zip(*t)
    return inters, slopes
\end{verbatim}

{\tt SamplingDistributions} takes a DataFrame with one row per live
birth, and {\tt iters}, the number of experiments to simulate.  It
uses {\tt ResampleRows} to resample the observed pregnancies.  We've
already seen {\tt SampleRows}, which chooses random rows from a
DataFrame.  {\tt thinkstats2} also provides {\tt ResampleRows}, which
returns a sample the same size as the original:
\index{DataFrame}
\index{resampling}

\begin{verbatim}
def ResampleRows(df):
    return SampleRows(df, len(df), replace=True)
\end{verbatim}

After resampling, we use the simulated sample to estimate parameters.
The result is two sequences: the estimated intercepts and estimated
slopes.
\index{parameter}

I summarize the sampling distributions by printing the standard
error and confidence interval:
\index{sampling distribution}

\begin{verbatim}
def Summarize(estimates, actual=None):
    mean = thinkstats2.Mean(estimates)
    stderr = thinkstats2.Std(estimates, mu=actual)
    cdf = thinkstats2.Cdf(estimates)
    ci = cdf.ConfidenceInterval(90)
    print('mean, SE, CI', mean, stderr, ci)
\end{verbatim}

{\tt Summarize} takes a sequence of estimates and the actual value.
It prints the mean of the estimates, the standard error and 
a 90\% confidence interval.
\index{standard error}
\index{confidence interval}

For the intercept, the mean estimate is 6.83, with standard error
0.07 and 90\% confidence interval (6.71, 6.94).  The estimated slope, in
more compact form, is 0.0174, SE 0.0028, CI (0.0126, 0.0220).
There is almost a factor of two between the low and high ends of
this CI, so it should be considered a rough estimate.

%inter 6.83039697331 6.83174035366
%SE, CI 0.0699814482068 (6.7146843084406846, 6.9447797068631871)
%slope 0.0174538514718 0.0173840926936
%SE, CI 0.00276116142884 (0.012635074392201724, 0.021975282350381781)

To visualize the sampling error of the estimate, we could plot
all of the fitted lines, or for a less cluttered representation,
plot a 90\% confidence interval for each age.  Here's the code:

\begin{verbatim}
def PlotConfidenceIntervals(xs, inters, slopes,
                            percent=90, **options):
    fys_seq = []
    for inter, slope in zip(inters, slopes):
        fxs, fys = thinkstats2.FitLine(xs, inter, slope)
        fys_seq.append(fys)

    p = (100 - percent) / 2
    percents = p, 100 - p
    low, high = thinkstats2.PercentileRows(fys_seq, percents)
    thinkplot.FillBetween(fxs, low, high, **options)
\end{verbatim}

{\tt xs} is the sequence of mother's age.  {\tt inters} and {\tt slopes}
are the estimated parameters generated by {\tt SamplingDistributions}.
{\tt percent} indicates which confidence interval to plot.

{\tt PlotConfidenceIntervals} generates a fitted line for each pair
of {\tt inter} and {\tt slope} and stores the results in a sequence,
\verb"fys_seq".  Then it uses {\tt PercentileRows} to select the
upper and lower percentiles of {\tt y} for each value of {\tt x}.
For a 90\% confidence interval, it selects the 5th and 95th percentiles.
{\tt FillBetween} draws a polygon that fills the space between two
lines.
\index{thinkplot}
\index{FillBetween}

\begin{figure}
% linear.py
\centerline{\includegraphics[height=2.5in]{figs/linear3.pdf}}
\caption{50\% and 90\% confidence intervals showing variability in the
  fitted line due to sampling error of {\tt inter} and {\tt slope}.}
\label{linear3}
\end{figure}

Figure~\ref{linear3} shows the 50\% and 90\% confidence
intervals for curves fitted to birth weight as a function of
mother's age.
  The vertical width of the region represents the effect of
sampling error; the effect is smaller for values near the mean and
larger for the extremes.


\section{Goodness of fit}
\label{goodness}
\index{goodness of fit}

There are several ways to measure the quality of a linear model, or
{\bf goodness of fit}.  One of the simplest is the standard deviation
of the residuals.
\index{standard deviation}
\index{model}

If you use a linear model to make predictions, {\tt Std(res)}
is the root mean squared error (RMSE) of your predictions.  For
example, if you use mother's age to guess birth weight, the RMSE of
your guess would be 1.40 lbs.
\index{birth weight}
\index{weight!birth}

If you guess birth weight without knowing the mother's age, the RMSE
of your guess is {\tt Std(ys)}, which is 1.41 lbs.  So in this
example, knowing a mother's age does not improve the predictions
substantially.
\index{prediction}

Another way to measure goodness of fit is  the {\bf
  coefficient of determination}, usually denoted $R^2$ and 
called ``R-squared'':
\index{coefficient of determination}
\index{r-squared}

\begin{verbatim}
def CoefDetermination(ys, res):
    return 1 - Var(res) / Var(ys)
\end{verbatim}

{\tt Var(res)} is the MSE of your guesses using the model,
{\tt Var(ys)} is the MSE without it.   So their ratio is the fraction
of MSE that remains if you use the model, and $R^2$ is the fraction
of MSE the model eliminates.
\index{MSE}

For birth weight and mother's age, $R^2$ is 0.0047, which means
that mother's age predicts about half of 1\% of variance in
birth weight.

There is a simple relationship between the coefficient of
determination and Pearson's coefficient of correlation: $R^2 = \rho^2$.
For example, if $\rho$ is 0.8 or -0.8, $R^2 = 0.64$.
\index{Pearson coefficient of correlation}

Although $\rho$ and $R^2$ are often used to quantify the strength of a
relationship, they are not easy to interpret in terms of predictive
power.  In my opinion, {\tt Std(res)} is the best representation
of the quality of prediction, especially if it is presented
in relation to {\tt Std(ys)}.
\index{coefficient of determination}
\index{r-squared}

For example, when people talk about the validity of the SAT
(a standardized test used for college admission in the U.S.) they
often talk about correlations between SAT scores and other measures of
intelligence.
\index{SAT}
\index{IQ}

According to one study, there is a Pearson correlation of
$\rho=0.72$ between total SAT scores and IQ scores, which sounds like
a strong correlation.  But $R^2 = \rho^2 = 0.52$, so SAT scores
account for only 52\% of variance in IQ.

IQ scores are normalized with {\tt Std(ys) = 15}, so

\begin{verbatim}
>>> var_ys = 15**2
>>> rho = 0.72
>>> r2 = rho**2
>>> var_res = (1 - r2) * var_ys
>>> std_res = math.sqrt(var_res)
10.4096
\end{verbatim}

So using SAT score to predict IQ reduces RMSE from 15 points to 10.4
points.  A correlation of 0.72 yields a reduction in RMSE of only
31\%.

If you see a correlation that looks impressive, remember that $R^2$ is
a better indicator of reduction in MSE, and reduction in RMSE is a
better indicator of predictive power.
\index{coefficient of determination}
\index{r-squared}
\index{prediction}


\section{Testing a linear model}

The effect of mother's age on birth weight is small, and has little
predictive power.  So is it possible that the apparent relationship
is due to chance?  There are several ways we might test the
results of a linear fit.
\index{birth weight}
\index{weight!birth}
\index{model}
\index{linear model}

One option is to test whether the apparent reduction in MSE is due to
chance.  In that case, the test statistic is $R^2$ and the null
hypothesis is that there is no relationship between the variables.  We
can simulate the null hypothesis by permutation, as in
Section~\ref{corrtest}, when we tested the correlation between
mother's age and birth weight.  In fact, because $R^2 = \rho^2$, a
one-sided test of $R^2$ is equivalent to a two-sided test of $\rho$.
We've already done that test, and found $p < 0.001$, so we conclude
that the apparent relationship between mother's age and birth weight
is statistically significant.
\index{null hypothesis}
\index{permutation}
\index{coefficient of determination}
\index{r-squared}
  \index{significant} \index{statistically significant}

Another approach is to test whether the apparent slope is due to chance.
The null hypothesis is that the slope is actually zero; in that case
we can model the birth weights as random variations around their mean.
Here's a HypothesisTest for this model:
\index{HypothesisTest}
\index{model}

\begin{verbatim}
class SlopeTest(thinkstats2.HypothesisTest):

    def TestStatistic(self, data):
        ages, weights = data
        _, slope = thinkstats2.LeastSquares(ages, weights)
        return slope

    def MakeModel(self):
        _, weights = self.data
        self.ybar = weights.mean()
        self.res = weights - self.ybar

    def RunModel(self):
        ages, _ = self.data
        weights = self.ybar + np.random.permutation(self.res)
        return ages, weights
\end{verbatim}

The data are represented as sequences of ages and weights.  The
test statistic is the slope estimated by {\tt LeastSquares}.
The model of the null hypothesis is represented by the mean weight
of all babies and the deviations from the mean.  To
generate simulated data, we permute the deviations and add them to
the mean.
\index{deviation}
\index{null hypothesis}
\index{permutation}

Here's the code that runs the hypothesis test:

\begin{verbatim}
    live, firsts, others = first.MakeFrames()
    live = live.dropna(subset=['agepreg', 'totalwgt_lb'])
    ht = SlopeTest((live.agepreg, live.totalwgt_lb))
    pvalue = ht.PValue()
\end{verbatim}

The p-value is less than $0.001$, so although the estimated
slope is small, it is unlikely to be due to chance.
\index{p-value}
\index{dropna}
\index{NaN}

Estimating the p-value by simulating the null hypothesis is strictly
correct, but there is a simpler alternative.  Remember that we already
computed the sampling distribution of the slope, in
Section~\ref{regest}.  To do that, we assumed that the observed slope
was correct and simulated experiments by resampling.
\index{null hypothesis}

Figure~\ref{linear4} shows the sampling distribution of the
slope, from Section~\ref{regest}, and the distribution of slopes
generated under the null hypothesis.  The sampling distribution
is centered about the estimated slope, 0.017 lbs/year, and the slopes
under the null hypothesis are centered around 0; but other than
that, the distributions are identical.  The distributions are
also symmetric, for reasons we will see in Section~\ref{CLT}.
\index{symmetric}
\index{sampling distribution}

\begin{figure}
% linear.py
\centerline{\includegraphics[height=2.5in]{figs/linear4.pdf}}
\caption{The sampling distribution of the estimated
slope and the distribution of slopes
generated under the null hypothesis.  The vertical lines are at 0
and the observed slope, 0.017 lbs/year.}
\label{linear4}
\end{figure}

So we could estimate the p-value two ways:
\index{p-value}

\begin{itemize}

\item Compute the probability that the slope under the null
hypothesis exceeds the observed slope.
\index{null hypothesis}

\item Compute the probability that the slope in the sampling
distribution falls below 0.  (If the estimated slope were negative,
we would compute the probability that the slope in the sampling
distribution exceeds 0.)

\end{itemize}

The second option is easier because we normally want to compute the
sampling distribution of the parameters anyway.  And it is a good
approximation unless the sample size is small {\em and} the
distribution of residuals is skewed.  Even then, it is usually good
enough, because p-values don't have to be precise.
\index{skewness}
\index{parameter}

Here's the code that estimates the p-value of the slope using the
sampling distribution:
\index{sampling distribution}

\begin{verbatim}
    inters, slopes = SamplingDistributions(live, iters=1001)
    slope_cdf = thinkstats2.Cdf(slopes)
    pvalue = slope_cdf[0]
\end{verbatim}

Again, we find $p < 0.001$.  


\section{Weighted resampling}
\label{weighted}

So far we have treated the NSFG data as if it were a representative
sample, but as I mentioned in Section~\ref{nsfg}, it is not.  The
survey deliberately oversamples several groups in order to
improve the chance of getting statistically significant results; that
is, in order to improve the power of tests involving these groups.
  \index{significant} \index{statistically significant}

This survey design is useful for many purposes, but it means that we
cannot use the sample to estimate values for the general
population without accounting for the sampling process.

For each respondent, the NSFG data includes a variable called {\tt
  finalwgt}, which is the number of people in the general population
the respondent represents.  This value is called a {\bf sampling
  weight}, or just ``weight.''
\index{sampling weight}
\index{weight}
\index{weighted resampling}
\index{resampling!weighted}

As an example, if you survey 100,000 people in a country of 300
million, each respondent represents 3,000 people.  If you oversample
one group by a factor of 2, each person in the oversampled
group would have a lower weight, about 1500.

To correct for oversampling, we can use resampling; that is, we
can draw samples from the survey using probabilities proportional
to sampling weights.  Then, for any quantity we want to estimate, we can
generate sampling distributions, standard errors, and confidence
intervals.  As an example, I will estimate mean birth weight with
and without sampling weights.
\index{standard error}
\index{confidence interval}
\index{birth weight}
\index{weight!birth}
\index{sampling distribution}
\index{oversampling}

In Section~\ref{regest}, we saw {\tt ResampleRows}, which chooses
rows from a DataFrame, giving each row the same probability.
Now we need to do the same thing using probabilities
proportional to sampling weights.
{\tt ResampleRowsWeighted} takes a DataFrame, resamples rows according
to the weights in {\tt finalwgt}, and returns a DataFrame containing
the resampled rows:
\index{DataFrame}
\index{resampling}

\begin{verbatim}
def ResampleRowsWeighted(df, column='finalwgt'):
    weights = df[column]
    cdf = Cdf(dict(weights))
    indices = cdf.Sample(len(weights))
    sample = df.loc[indices]
    return sample
\end{verbatim}

{\tt weights} is a Series; converting it to a dictionary makes
a map from the indices to the weights.  In {\tt cdf} the values
are indices and the probabilities are proportional to the
weights.

{\tt indices} is a sequence of row indices; {\tt sample} is a
DataFrame that contains the selected rows.  Since we sample with
replacement, the same row might appear more than once.  \index{Cdf}
\index{replacement}

Now we can compare the effect of resampling with and without
weights.  Without weights, we generate the sampling distribution
like this:
\index{sampling distribution}

\begin{verbatim}
    estimates = [ResampleRows(live).totalwgt_lb.mean()
                 for _ in range(iters)]
\end{verbatim}

With weights, it looks like this:

\begin{verbatim}
    estimates = [ResampleRowsWeighted(live).totalwgt_lb.mean()
                 for _ in range(iters)]
\end{verbatim}

The following table summarizes the results:

\begin{center}
\begin{tabular}{|l|c|c|c|}
\hline
                    &  mean birth   & standard  &  90\% CI  \\ 
                    &  weight (lbs) & error     &           \\ 
\hline
Unweighted          &  7.27  &  0.014  &  (7.24, 7.29)  \\ 
Weighted            &  7.35  &  0.014  &  (7.32, 7.37)  \\ 
\hline
\end{tabular}
\end{center}

%mean 7.26580789518
%stderr 0.0141683527792
%ci (7.2428565501217079, 7.2890814917127074)
%mean 7.34778034718
%stderr 0.0142738972319
%ci (7.3232804012858885, 7.3704916897506925)

In this example, the effect of weighting is small but non-negligible.
The difference in estimated means, with and without weighting, is
about 0.08 pounds, or 1.3 ounces.  This difference is substantially
larger than the standard error of the estimate, 0.014 pounds, which
implies that the difference is not due to chance.
\index{standard error}
\index{confidence interval}


\section{Exercises}

A solution to this exercise is in \verb"chap10soln.ipynb"

\begin{exercise}

Using the data from the BRFSS, compute the linear least squares
fit for log(weight) versus height.
How would you best present the estimated parameters for a model
like this where one of the variables is log-transformed?
If you were trying to guess
someone's weight, how much would it help to know their height?
\index{Behavioral Risk Factor Surveillance System}
\index{BRFSS}
\index{model}

Like the NSFG, the BRFSS oversamples some groups and provides
a sampling weight for each respondent.  In the BRFSS data, the variable
name for these weights is {\tt totalwt}.
Use resampling, with and without weights, to estimate the mean height
of respondents in the BRFSS, the standard error of the mean, and a
90\% confidence interval.  How much does correct weighting affect the
estimates?
\index{confidence interval}
\index{standard error}
\index{oversampling}
\index{sampling weight}
\end{exercise}


\section{Glossary}

\begin{itemize}

\item linear fit: a line intended to model the relationship between
variables.  \index{linear fit}

\item least squares fit: A model of a dataset that minimizes the
sum of squares of the residuals.
\index{least squares fit}

\item residual: The deviation of an actual value from a model.
\index{residuals}

\item goodness of fit: A measure of how well a model fits data.
\index{goodness of fit}

\item coefficient of determination: A statistic intended to
quantify goodness of fit.
\index{coefficient of determination}

\item sampling weight: A value associated with an observation in a
  sample that indicates what part of the population it represents.
\index{sampling weight}

\end{itemize}

