

\chapter{회귀 (Regression)}
\label{regression}

앞장에서 선형최소적합은 {\bf 회귀 (regression)}의 한 사례다. 회귀는 어떤 종류의 데이터를 어떤 종류의 모형에도 적합하는 더 일반적인 문제다. ``회귀 (regression)'' 용어 사용은 역사적 우연이다; 단어의 본래 의미와는 간접적으로 연관된다.
\index{모형 (model)}
\index{회귀 (regression)}

회귀분석의 목적은 {\bf 종속변수 (dependent variables)}라고 불리는 변수집합과 {\bf 설명변수 (explanatory variables)}라고 불리는 또 다른 집합변수 사이 관계를 기술하는 것이다.
\index{설명변수 (explanatory variable)}
\index{종속변수 (dependent variable)}

앞장에서 종속변수로 출생 체중을 예측하는데 설명변수로 산모 연령을 사용했다.
단지 종속변수가 하나 설명변수도 하나라면, {\bf 단순회귀(simple regression)}가 된다. 이번 장에서는 하나 이상 설명변수를 갖는 {\bf 다중회귀(multiple regression)}로 옮겨간다. 만약 종속변수가 하나이상이라면, 다변량회귀(multivariate regression)이 된다.
\index{출생 체중 (birth weight)}
\index{체중 (weight)!출생 (birth)}
\index{단순회귀 (simple regression)}
\index{다중회귀 (multiple regression)}


만약 종속변수와 설명변수 간 관계가 선형이면, {\bf 선형회귀 (linear regression)}가 된다. 예를 들어, 종속변수가 $y$이고, 설명변수가 $x_1$과 $x_2$라면, 
다음과 같이 선형회귀모형을 작성할 수 있다.
%
\[ y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \eps \]
%

여기서 $\beta_0$는 절편, $\beta_1$은 $x_1$과 연관된 모수, $\beta_2$는 
$x_2$와 연관된 모수, 그리고 $\eps$는 확률변동 혹은 다른 미지 요인으로 인한 잔차다.
\index{회귀모형 (regression model)}
\index{선형회귀 (linear regression)}

$y$에 대한 시퀀스 값과 $x_1$과 $x_2$에 대한 시퀀스 값이 주어지면, $\eps^2$ 합을 최소화하는 $\beta_0$, $\beta_1$, $\beta_2$ 모수를 찾을 수 있다. 이 과정을 {\bf 보통최소제곱(ordinary least squares)}이라고 부른다. 
연산은 {\tt thinkstats2.LeastSquare}와 유사하지만, 하나 이상의 설명변수를 다루도록 일반화되었다. 자세한 정보는 웹사이트를 참조한다. \url{https://en.wikipedia.org/wiki/Ordinary_least_squares}
\index{설명변수 (explanatory variable)}
\index{보통최소제곱 (ordinary least squares)}
\index{모수 (parameter)}

이번 장에서 사용되는 코드는 {\tt regression.py}에 있다.
코드를 다운로드하고 작업하는 것에 대한 정보는 ~\ref{code}을 참조한다.

\section{StatsModels}
\label{statsmodels}

앞장에서 가독성이 좋은 단순선형회귀모형을 구현한 {\tt thinkstats2.LeastSquares}를 제시했다. 다중회귀에 대해서 StatsModels로 전환한다. 파이썬 팩키지로 몇가지 형태 회귀분석와 다른 분석 기능을 제공한다. 만약 아나콘다(Anaconda)를 사용하고 있다면, 이미 StatsModels이 설치되어 있다; 그렇지 않다면, 설치해야할지 모른다.
\index{아나콘다 (Anaconda)}

예제로, StatModels을 가지고 앞장 모형을 실행한다.
\index{StatsModels}
\index{모형 (model)}

\begin{verbatim}
    import statsmodels.formula.api as smf

    live, firsts, others = first.MakeFrames()
    formula = 'totalwgt_lb ~ agepreg'
    model = smf.ols(formula, data=live)
    results = model.fit()
\end{verbatim}

{\tt statsmodels}은 인터페이스(APIs) 두개를 제공한다; ``formula'' API는 문자열로 종속변화 설명변수를 식별한다.
{\tt patsy}라는 구문(syntax)을 사용한다; 상기 예제에서, \verb"~" 연산자가 왼편에 종속변수와 오른편에 설명변수를 구별한다.
\index{설명변수 (explanatory variable)}
\index{종속변수 (dependent variable)}
\index{Patsy}

{\tt smf.ols}는 인자로 문자열 공식과 데이터프레임 {\tt live}를 받고,
모형을 표현하는 OLS객체를 반환한다.
{\tt ols} 는 ``ordinary least squares'' 약자다.
\index{데이터프레임 (DataFrame)}
\index{모형 (model)}
\index{보통최소제곱 (ordinary least squares)}

{\tt fit}메쏘드는 모형을 데이터에 적합하고 결과를 담고 있는 객체 RegressionResults를 반환한다.
\index{RegressionResults}

결과는 또한 속성(attribute)으로도 접근가능하다.
{\tt params}은 시리즈로 변수명을 모수에 매핑한다.
그래서, 다음과 같은 절편과 기울기를 얻을 수 있다.
\index{시리즈 (Series)}

\begin{verbatim}
    inter = results.params['Intercept']
    slope = results.params['agepreg']
\end{verbatim}

추정한 모수는 6.83 와 0.0175으로 {\tt LeastSquares}로 추정한 것과 동일한다.
\index{모수 (parameter)}

{\tt pvalues}는 시리즈로 변수명과 연관된 p-값을 매핑한다.
그래서 추정한 기울기가 통계적으로 유의한지 점검할 수 있다.
\index{p-값 (p-value)}
\index{유의성 (significant)} 
\index{통계적 유의성 (statistically significant)}

\begin{verbatim}
    slope_pvalue = results.pvalues['agepreg']
\end{verbatim}

{\tt agepreg}와 연관된 p-값은 {\tt 5.7e-11} 으로 예상한 것처럼 $0.001$ 보다 적다.
\index{연령 (age)}

{\tt results.rsquared}는 $R^2$ 정보를 담고 있는데 $0.0047$이다.  
{\tt results}는 또한 전체 모형과 연관된 p-값인 \verb"f_pvalue"도 제공하는데 $R^2$가 통계적으로 유의한가를 점검하는 검정과 유사하다. 
\index{모형 (model)}
\index{결정계수 (coefficient of determination)}
\index{r-제곱 (r-squared)}

{\tt results}는 잔차 시퀀스인 {\tt resid}와 {\tt agepreg}에 상응하는 적합한 값 시퀀스 {\tt fittedvalues}도 제공한다.
\index{잔차 (residuals)}

{\tt results} 객체는 {\tt summary()} 메쏘드도 제공하는데,
가독성이 좋은 형식으로 결과를 표현한다.

\begin{verbatim}
    print(results.summary())
\end{verbatim}
하지만 (아직) 관련되지도 않은 많은 정보를 출력한다.
그래서 {\tt SummarizeResults}로 불리는 더 간단한 함수를 사용한다.
다음에 모형 결과가 있다.

\begin{verbatim}
Intercept       6.83    (0)
agepreg         0.0175  (5.72e-11)
R^2 0.004738
Std(ys) 1.408
Std(res) 1.405
\end{verbatim}

{\tt Std(ys)}는 만약 어떤 설명변수 도움없이 출생 체중을 추측해야 한다면 갖게될 종속변수 표준편차 RMSE가 된다.
{\tt Std(res)}는 만약 추측에 산모 연령 정보를 안다면 갖게될 잔차 표준편차 RMSE가 된다. 
이미 살펴봤듯이, 산모 연령을 아는 것이 그다지 예측에 향상을 가져오지는 않는다.

\index{표준편차 (standard deviation)}
\index{출생체중 (birth weight)}
\index{체중 (weight)!출생 (birth)}
\index{설명변수 (explanatory variable)}
\index{종속변수 (dependent variable)}
\index{RMSE}
\index{예측력 (predictive power)}


\section{다중회귀 (Multiple regression)}
\label{multiple}

~\ref{birth_weights}절에서 첫째 아이 체중이 첫째가 아닌 아이들 체중보다 가벼운 경향이 있고, 그 효과가 통계적 유의적성이 있다는 것을 봤다.
하지만, 이상한 결과로 이유는 첫번째 아이 체중이 더 가볍다고 할 수 있는 분명한 메커니즘(mechanism)은 없다. 그래서 이러한 관계가 {\bf 거짓(spurious)}인지 궁금하다.
\index{다중회귀 (multiple regression)}
\index{거짓관계(spurious relationship)}

사실 이 효과에 대한 가능한 설명이 있기는 하다. 출생 체중이 산모 연령에 의존한 것을 봤다. 그리고 첫째 아이 산모는 첫째가 아닌 아이 산모보다 더 어리다는 것을 예상할 수 있다.
\index{체중 (weight)}
\index{연령 (age)}
계산 몇번으로 상기 설명이 그럴듯한지 점검할 수 있다.
그리고 나서, 다중회귀를 사용해서 좀더 자세히 조사할 것이다. 먼저 체중 차이가 얼마나 큰지 살펴본다.

\begin{verbatim}
diff_weight = firsts.totalwgt_lb.mean() - others.totalwgt_lb.mean()
\end{verbatim}

첫째 아이는 0.125 lbs, 즉 2 온스 가볍다. 그리고 연령 차이는 다음과 같다.

\begin{verbatim}
diff_age = firsts.agepreg.mean() - others.agepreg.mean()
\end{verbatim}

첫째 아이 산모 나이가 3.59 년 더 젊다.
다시 선형 모형을 돌리게 되면, 연령 함수로 출생체중에 변화값를 얻는다.
\index{출생 체중 (birth weight)}
\index{체중 (weight)!출생 (birth)}

\begin{verbatim}
results = smf.ols('totalwgt_lb ~ agepreg', data=live).fit()
slope = results.params['agepreg']
\end{verbatim}

기울기는 년당 0.175 파운드다. 만약 기울기를 연령 차이와 곱하게 되면, 산모 연령 때문에 첫째 아이와 첫째가 아닌 아이들에 대한 출생 체중 평균 차이를 얻게된다. 

\begin{verbatim}
slope * diff_age
\end{verbatim}

결과는 0.063 으로 관측 차이의 약 절반이다. 
그래서 잠정적으로 출생 체중에 관측 차이는 부분적으로 산모 연령 차이로 설명될 수 있다고 결론낸다.

다중 회귀를 사용해서, 관계를 좀더 체계적으로 탐색할 수 있다.
\index{다중 회귀 (multiple regression)}

\begin{verbatim}
    live['isfirst'] = live.birthord == 1
    formula = 'totalwgt_lb ~ isfirst'
    results = smf.ols(formula, data=live).fit()
\end{verbatim}

첫번째 행은 {\tt isfirst}라는 새로운 칼럼(열)을 생성한다. 첫번째 아이는 참(True), 첫번째 아이가 아니면 거싲(False)다. 그리고 나서, 설명변수로 {\tt isfirst}를 사용해서 모형에 적합한다.
\index{모형 (model)}
\index{설명 변수 (explanatory variable)}

다음에 결과가 있다.

\begin{verbatim}
Intercept         7.33   (0)
isfirst[T.True]  -0.125  (2.55e-05)
R^2 0.00196
\end{verbatim}

{\tt isfirst}는 부울(boolean)이기 때문에, \tt ols}는 이 변수를 
{\bf 범주형 변수 (categorical variable)}로 처리한다. 의미하는 바는 값이 참(true)과 거짓(false) 같은 범주에 속하기 때문에 숫자로 처리되면 안된다는 것이다.
추정된 모수는 {\tt isfirst}가 참일때 출생 체중에 대한 효과다. 그래서 결과 -0.125 lbs 는 첫번째 아이와 첫째가 아닌 아이들 간에 출생체중 차이다.
\index{출생 체중 (birth weight)}
\index{체중 (weight)!출생 (birth)}
\index{범주형 변수 (categorical variable)}
\index{부울 (boolean)}

기울기와 절편이 통계적 유의성이 있다. 의미하는 바는 우연으로 발생한 것 같지는 않다. 하지만, 모형 $R^2$ 값이 작다. 의미하는 바는 {\tt isfirst} 가 출생체중 변동 상당부분을 설명하지는 못한다는 것이다.
\index{결정계수 (coefficient of determination)}
\index{r-제곱 (r-squared)}

결과는 {\tt agepreg}와 비슷하다.

\begin{verbatim}
Intercept       6.83    (0)
agepreg         0.0175  (5.72e-11)
R^2 0.004738
\end{verbatim}

다시 한번, 모수는 통계적 유의성이 있으나, $R^2$ 값은 낮다.
\index{결정계수 (coefficient of determination)}
\index{r-제곱 (r-squared)}

상기 모형은 지금까지 살펴본 결과를 다시 확인해 준다. 하지만, 이제 변수 두개를 넣어서 모형을 적합할 수 있다. 구문 공식 \verb"totalwgt_lb ~ isfirst + agepreg"을 대입하면, 다음을 얻는다:

\begin{verbatim}
Intercept        6.91    (0)
isfirst[T.True] -0.0698  (0.0253)
agepreg          0.0154  (3.93e-08)
R^2 0.005289
\end{verbatim}

조합 모형에서, {\tt isfirst}에 대한 모수는 약 절반 작아졌다. 의미하는 것은 {\tt isfirst}의 외관효과가 사실 {\tt agepreg}으로 설명이 된다는 것이다.
그리고 {\tt isfirst}에 대한 p-값은 약 2.5\%로, 통계적 유의성 경계선상에 있다.
\index{p-값 (p-value)}
\index{모형 (model)}

모형에 $R^2$값이 약간 더 크다. 따라서 변수 두개가 혼자보다 출산 체중에 변동성을 더 많이 (하지만 그다지 많지는 않다) 설명하는 것을 나타낸다.

\index{출생 체중 (birth weight)}
\index{체중 (weight)!출생 (birth)}
\index{결정계수 (coefficient of determination)}
\index{r-제곱 (r-squared)}


\section{비선형 관계 (Nonlinear relationships)}
\label{nonlinear}

Remembering that the contribution of {\tt agepreg} might be nonlinear,
we might consider adding a variable to capture more of this
relationship.  One option is to create a column, {\tt agepreg2},
that contains the squares of the ages:
\index{nonlinear}

\begin{verbatim}
    live['agepreg2'] = live.agepreg**2
    formula = 'totalwgt_lb ~ isfirst + agepreg + agepreg2'
\end{verbatim}

Now by estimating parameters for {\tt agepreg} and {\tt agepreg2},
we are effectively fitting a parabola:

\begin{verbatim}
Intercept        5.69     (1.38e-86)
isfirst[T.True] -0.0504   (0.109)
agepreg          0.112    (3.23e-07)
agepreg2        -0.00185  (8.8e-06)
R^2 0.007462
\end{verbatim}

The parameter of {\tt agepreg2} is negative, so the parabola
curves downward, which is consistent with the shape of the lines
in Figure~\ref{linear2}.
\index{parabola}

The quadratic model of {\tt agepreg} accounts for more of the
variability in birth weight; the parameter for {\tt isfirst}
is smaller in this model, and no longer statistically significant.
\index{birth weight}
\index{weight!birth}
\index{quadratic model}
\index{model}
  \index{significant} \index{statistically significant}

Using computed variables like {\tt agepreg2} is a common way to
fit polynomials and other functions to data.  
This process is still considered linear
regression, because the dependent variable is a linear function of
the explanatory variables, regardless of whether some variables
are nonlinear functions of others.
\index{explanatory variable}
\index{dependent variable}
\index{nonlinear}

The following table summarizes the results of these regressions:

\begin{center}
\begin{tabular}{|l|c|c|c|c|}
\hline & isfirst & agepreg & agepreg2 & $R^2$ \\ \hline
Model 1 & -0.125 * & -- & -- & 0.002 \\
Model 2 & -- & 0.0175 * & -- & 0.0047 \\
Model 3 & -0.0698 (0.025) & 0.0154 * & -- & 0.0053 \\
Model 4 & -0.0504 (0.11) & 0.112 * & -0.00185 * & 0.0075 \\
\hline
\end{tabular}
\end{center}

The columns in this table are the explanatory variables and
the coefficient of determination, $R^2$.  Each entry is an estimated
parameter and either a p-value in parentheses or an asterisk to
indicate a p-value less that 0.001.
\index{p-value}
\index{coefficient of determination}
\index{r-squared}
\index{explanatory variable}

We conclude that the apparent difference in birth weight
is explained, at least in part, by the difference in mother's age.
When we include mother's age in the model, the effect of
{\tt isfirst} gets smaller, and the remaining effect might be
due to chance.
\index{age}

In this example, mother's age acts as a {\bf control variable};
including {\tt agepreg} in the model ``controls for'' the
difference in age between first-time mothers and others, making
it possible to isolate the effect (if any) of {\tt isfirst}. 
\index{control variable}


\section{Data mining}
\label{mining}

So far we have used regression models for explanation; for example,
in the previous section we discovered that an apparent difference
in birth weight is actually due to a difference in mother's age.
But the $R^2$ values of those models is very low, which means that
they have little predictive power.  In this section we'll try to
do better.
\index{birth weight}
\index{weight!birth}
\index{regression model}
\index{coefficient of determination}
\index{r-squared}

Suppose one of your co-workers is expecting a baby and
there is an office pool to guess the baby's birth weight (if you are
not familiar with betting pools, see
\url{https://en.wikipedia.org/wiki/Betting_pool}).
\index{betting pool}

Now suppose that you {\em really} want to win the pool.  What could
you do to improve your chances?  Well, 
the NSFG dataset includes 244 variables about each pregnancy and another
3087 variables about each respondent.  Maybe some of those variables
have predictive power.  To find out which ones are most useful,
why not try them all?
\index{NSFG}

Testing the variables in the pregnancy table is easy, but in order to
use the variables in the respondent table, we have to match up each
pregnancy with a respondent.  In theory we could iterate through the
rows of the pregnancy table, use the {\tt caseid} to find the
corresponding respondent, and copy the values from the
correspondent table into the pregnancy table.  But that would be slow.
\index{join}
\index{SQL}

A better option is to recognize this process as a {\bf join} operation
as defined in SQL and other relational database languages (see
\url{https://en.wikipedia.org/wiki/Join_(SQL)}).  Join is implemented
as a DataFrame method, so we can perform the operation like this:
\index{DataFrame}

\begin{verbatim}
    live = live[live.prglngth>30]
    resp = chap01soln.ReadFemResp()
    resp.index = resp.caseid
    join = live.join(resp, on='caseid', rsuffix='_r')
\end{verbatim}

The first line selects records for pregnancies longer than 30 weeks,
assuming that the office pool is formed several weeks before the
due date.
\index{betting pool}

The next line reads the respondent file.  The result is a DataFrame
with integer indices; in order to look up respondents efficiently,
I replace {\tt resp.index} with {\tt resp.caseid}. 

The {\tt join} method is invoked on {\tt live}, which is considered
the ``left'' table, and passed {\tt resp}, which is the ``right'' table.
The keyword argument {\tt on} indicates the variable used to match up
rows from the two tables.

In this example some column names appear in both tables,
so we have to provide {\tt rsuffix}, which is a string that will be
appended to the names of overlapping columns from the right table.
For example, both tables have a column named {\tt race} that encodes
the race of the respondent.  The result of the join contains two
columns named {\tt race} and \verb"race_r".
\index{race}

The pandas implementation is fast.  Joining the NSFG tables takes
less than a second on an ordinary desktop computer.
Now we can start testing variables.
\index{pandas}
\index{join}

\begin{verbatim}
    t = []
    for name in join.columns:
        try:
            if join[name].var() < 1e-7:
                continue

            formula = 'totalwgt_lb ~ agepreg + ' + name
            model = smf.ols(formula, data=join)
            if model.nobs < len(join)/2:
                continue

            results = model.fit()
        except (ValueError, TypeError):
            continue

        t.append((results.rsquared, name))
\end{verbatim}

For each variable we construct a model, compute $R^2$, and append
the results to a list.  The models all include {\tt agepreg}, since
we already know that it has some predictive power.
\index{model}
\index{coefficient of determination}
\index{r-squared}

I check that each explanatory variable has some variability; otherwise
the results of the regression are unreliable.  I also check the number
of observations for each model.  Variables that contain a large number
of {\tt nan}s are not good candidates for prediction.
\index{explanatory variable}
\index{NaN}

For most of these variables, we haven't done any cleaning.  Some of them
are encoded in ways that don't work very well for linear regression.
As a result, we might overlook some variables that would be useful if
they were cleaned properly.  But maybe we will find some good candidates.
\index{cleaning}


\section{Prediction}

The next step is to sort the results and select the variables that
yield the highest values of $R^2$.
\index{prediction}

\begin{verbatim}
    t.sort(reverse=True)
    for mse, name in t[:30]:
        print(name, mse)
\end{verbatim}

The first variable on the list is \verb"totalwgt_lb",
followed by \verb"birthwgt_lb".  Obviously, we can't use birth
weight to predict birth weight.
\index{birth weight}
\index{weight!birth}

Similarly {\tt prglngth} has useful predictive power, but for the
office pool we assume pregnancy length (and the related variables)
are not known yet.
\index{predictive power}
\index{pregnancy length}

The first useful predictive variable is {\tt babysex} which indicates
whether the baby is male or female.  In the NSFG dataset, boys are
about 0.3 lbs heavier.  So, assuming that the sex of the baby is
known, we can use it for prediction.
\index{sex}

Next is {\tt race}, which indicates whether the respondent is white,
black, or other.  As an explanatory variable, race can be problematic.
In datasets like the NSFG, race is correlated with many other
variables, including income and other socioeconomic factors.  In a
regression model, race acts as a {\bf proxy variable},
so apparent correlations with race are often caused, at least in
part, by other factors.
\index{explanatory variable}
\index{race}

The next variable on the list is {\tt nbrnaliv}, which indicates
whether the pregnancy yielded multiple births.  Twins and triplets
tend to be smaller than other babies, so if we know whether our
hypothetical co-worker is expecting twins, that would help.
\index{multiple birth}

Next on the list is {\tt paydu}, which indicates whether the
respondent owns her home.  It is one of several income-related
variables that turn out to be predictive.  In datasets like the NSFG,
income and wealth are correlated with just about everything.  In this
example, income is related to diet, health, health care, and other
factors likely to affect birth weight.
\index{birth weight}
\index{weight!birth}
\index{income}
\index{wealth}

Some of the other variables on the list are things that would not
be known until later, like {\tt bfeedwks}, the number of weeks
the baby was breast fed.  We can't use these variables for prediction,
but you might want to speculate on reasons
{\tt bfeedwks} might be correlated with birth weight.

Sometimes you start with a theory and use data to test it.  Other
times you start with data and go looking for possible theories.
The second approach, which this section demonstrates, is
called {\bf data mining}.  An advantage of data mining is that it
can discover unexpected patterns.  A hazard is that many of the
patterns it discovers are either random or spurious.
\index{theory}
\index{data mining}

Having identified potential explanatory variables, I tested a few
models and settled on this one:
\index{model}
\index{explanatory variable}

\begin{verbatim}
    formula = ('totalwgt_lb ~ agepreg + C(race) + babysex==1 + '
               'nbrnaliv>1 + paydu==1 + totincr')
    results = smf.ols(formula, data=join).fit()
\end{verbatim}

This formula uses some syntax we have not seen yet:
{\tt C(race)} tells the formula parser (Patsy) to treat race as a
categorical variable, even though it is encoded numerically.
\index{Patsy}
\index{categorical variable}

The encoding for {\tt babysex} is 1 for male, 2 for female; writing
{\tt babysex==1} converts it to boolean, True for male and false for
female.
\index{boolean}

Similarly {\tt nbrnaliv>1} is True for multiple births and 
{\tt paydu==1} is True for respondents who own their houses.

{\tt totincr} is encoded numerically from 1-14, with each increment
representing about \$5000 in annual income.  So we can treat these
values as numerical, expressed in units of \$5000.
\index{income}

Here are the results of the model:

\begin{verbatim}
Intercept               6.63    (0)
C(race)[T.2]            0.357   (5.43e-29)
C(race)[T.3]            0.266   (2.33e-07)
babysex == 1[T.True]    0.295   (5.39e-29)
nbrnaliv > 1[T.True]   -1.38    (5.1e-37)
paydu == 1[T.True]      0.12    (0.000114)
agepreg                 0.00741 (0.0035)
totincr                 0.0122  (0.00188)
\end{verbatim}

The estimated parameters for race are larger than I expected,
especially since we control for income.  The encoding
is 1 for black, 2 for white, and 3 for other.  Babies of black
mothers are lighter than babies of other races by 0.27--0.36 lbs.
\index{control variable}
\index{race}

As we've already seen, boys are heavier by about 0.3 lbs;
twins and other multiplets are lighter by 1.4 lbs.
\index{weight}

People who own their homes have heavier babies by about 0.12 lbs,
even when we control for income.  The parameter for mother's
age is smaller than what we saw in Section~\ref{multiple}, which
suggests that some of the other variables are correlated with
age, probably including {\tt paydu} and {\tt totincr}.
\index{income}

All of these variables are statistically significant, some with
very low p-values, but 
$R^2$ is only 0.06, still quite small.
RMSE without using the model is 1.27 lbs; with the model it drops
to 1.23.  So your chance of winning the pool is not substantially
improved.  Sorry!
\index{p-value}
\index{model}
\index{coefficient of determination}
\index{r-squared}
  \index{significant} \index{statistically significant}



\section{Logistic regression}

In the previous examples, some of the explanatory variables were
numerical and some categorical (including boolean).  But the dependent
variable was always numerical.
\index{explanatory variable}
\index{dependent variable}
\index{categorical variable}

Linear regression can be generalized to handle other kinds of
dependent variables.  If the dependent variable is boolean, the
generalized model is called {\bf logistic regression}.  If the dependent
variable is an integer count, it's called {\bf Poisson
regression}.
\index{model}
\index{logistic regression}
\index{Poisson regression}
\index{boolean}

As an example of logistic regression, let's consider a variation
on the office pool scenario.
Suppose
a friend of yours is pregnant and you want to predict whether the
baby is a boy or a girl.  You could use data from the NSFG to find
factors that affect the ``sex ratio'', which is conventionally
defined to be the probability
of having a boy.
\index{betting pool}
\index{sex}

If you encode the dependent variable numerically, for example 0 for a
girl and 1 for a boy, you could apply ordinary least squares, but
there would be problems.  The linear model might be something like
this:
%
\[ y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \eps \]
%
Where $y$ is the dependent variable, and $x_1$ and $x_2$ are
explanatory variables.  Then we could find the parameters that
minimize the residuals.
\index{regression model}
\index{explanatory variable}
\index{dependent variable}
\index{ordinary least squares}

The problem with this approach is that it produces predictions that
are hard to interpret.  Given estimated parameters and values for
$x_1$ and $x_2$, the model might predict $y=0.5$, but the only
meaningful values of $y$ are 0 and 1.
\index{parameter}

It is tempting to interpret a result like that as a probability; for
example, we might say that a respondent with particular values of
$x_1$ and $x_2$ has a 50\% chance of having a boy.  But it is also
possible for this model to predict $y=1.1$ or $y=-0.1$, and those
are not valid probabilities.
\index{probability}

Logistic regression avoids this problem by expressing predictions in
terms of {\bf odds} rather than probabilities.  If you are not
familiar with odds, ``odds in favor'' of an event is the ratio of the
probability it will occur to the probability that it will not.
\index{odds}

So if I think my team has a 75\% chance of winning, I would
say that the odds in their favor are three to one, because
the chance of winning is three times the chance of losing.

Odds and probabilities are different representations of the same
information.  Given a probability, you can compute the odds like this:

\begin{verbatim}
    o = p / (1-p)
\end{verbatim}

Given odds in favor, you can convert to
probability like this:

\begin{verbatim}
    p = o / (o+1)
\end{verbatim}

Logistic regression is based on the following model:
%
\[ \log o = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \eps \]
%
Where $o$ is the odds in favor of a particular outcome; in the
example, $o$ would be the odds of having a boy.
\index{regression model}

Suppose we have estimated the parameters $\beta_0$, $\beta_1$, and
$\beta_2$ (I'll explain how in a minute).  And suppose we are given
values for $x_1$ and $x_2$.  We can compute the predicted value of
$\log o$, and then convert to a probability:

\begin{verbatim}
    o = np.exp(log_o)
    p = o / (o+1)
\end{verbatim}

So in the office pool scenario we could compute the predictive
probability of having a boy.  But how do we estimate the parameters?
\index{parameter}


\section{Estimating parameters}

Unlike linear regression, logistic regression does not have a
closed form solution, so it is solved by guessing an initial
solution and improving it iteratively.
\index{logistic regression}
\index{closed form}

The usual goal is to find the maximum-likelihood estimate (MLE),
which is the set of parameters that maximizes the likelihood of the
data.  For example, suppose we have the following data:
\index{MLE}
\index{maximum likelihood estimator}

\begin{verbatim}
>>> y = np.array([0, 1, 0, 1])
>>> x1 = np.array([0, 0, 0, 1])
>>> x2 = np.array([0, 1, 1, 1])
\end{verbatim}

And we start with the initial guesses $\beta_0=-1.5$, $\beta_1=2.8$,
and $\beta_2=1.1$:

\begin{verbatim}
>>> beta = [-1.5, 2.8, 1.1]
\end{verbatim}

Then for each row we can compute \verb"log_o":

\begin{verbatim}
>>> log_o = beta[0] + beta[1] * x1 + beta[2] * x2 
[-1.5 -0.4 -0.4  2.4]
\end{verbatim}

And convert from log odds to probabilities:
\index{log odds}

\begin{verbatim}
>>> o = np.exp(log_o)
[  0.223   0.670   0.670  11.02  ]

>>> p = o / (o+1)
[ 0.182  0.401  0.401  0.916 ]
\end{verbatim}

Notice that when \verb"log_o" is greater than 0, {\tt o}
is greater than 1 and {\tt p} is greater than 0.5.

The likelihood of an outcome is {\tt p} when {\tt y==1} and {\tt 1-p}
when {\tt y==0}.  For example, if we think the probability of a boy is
0.8 and the outcome is a boy, the likelihood is 0.8; if
the outcome is a girl, the likelihood is 0.2.  We can compute that
like this:
\index{likelihood}

\begin{verbatim}
>>> likes = y * p + (1-y) * (1-p)
[ 0.817  0.401  0.598  0.916 ]
\end{verbatim}

The overall likelihood of the data is the product of {\tt likes}:

\begin{verbatim}
>>> like = np.prod(likes)
0.18
\end{verbatim}

For these values of {\tt beta}, the likelihood of the data is 0.18.
The goal of logistic regression is to find parameters that maximize
this likelihood.  To do that, most statistics packages use an
iterative solver like Newton's method (see
\url{https://en.wikipedia.org/wiki/Logistic_regression#Model_fitting}).
\index{Newton's method}
\index{iterative solver}


\section{Implementation}
\label{implementation}

StatsModels provides an implementation of logistic regression
called {\tt logit}, named for the function that converts from
probability to log odds.  To demonstrate its use, I'll look for
variables that affect the sex ratio.
\index{StatsModels}
\index{sex ratio}
\index{logit function}

Again, I load the NSFG data and select pregnancies longer than
30 weeks:

\begin{verbatim}
    live, firsts, others = first.MakeFrames()
    df = live[live.prglngth>30]
\end{verbatim}

{\tt logit} requires the dependent variable to be binary (rather than
boolean), so I create a new column named {\tt boy}, using {\tt
  astype(int)} to convert to binary integers:
\index{dependent variable}
\index{boolean}
\index{binary}

\begin{verbatim}
    df['boy'] = (df.babysex==1).astype(int)
\end{verbatim}

Factors that have been found to affect sex ratio include parents'
age, birth order, race, and social status.  We can use logistic
regression to see if these effects appear in the NSFG data.  I'll
start with the mother's age:
\index{age}
\index{race}

\begin{verbatim}
    import statsmodels.formula.api as smf

    model = smf.logit('boy ~ agepreg', data=df)
    results = model.fit()
    SummarizeResults(results)
\end{verbatim}

{\tt logit} takes the same arguments as {\tt ols}, a formula
in Patsy syntax and a DataFrame.  The result is a Logit object
that represents the model.  It contains attributes called
{\tt endog} and {\tt exog} that contain the {\bf endogenous
variable}, another name for the dependent variable,
and the {\bf exogenous variables}, another name for the
explanatory variables.  Since they are NumPy arrays, it is
sometimes convenient to convert them to DataFrames:
\index{NumPy}
\index{pandas}
\index{DataFrame}
\index{explanatory variable}
\index{dependent variable}
\index{exogenous variable}
\index{endogenous variable}
\index{Patsy}

\begin{verbatim}
    endog = pandas.DataFrame(model.endog, columns=[model.endog_names])
    exog = pandas.DataFrame(model.exog, columns=model.exog_names)
\end{verbatim}

The result of {\tt model.fit} is a BinaryResults object, which is
similar to the RegressionResults object we got from {\tt ols}.
Here is a summary of the results:

\begin{verbatim}
Intercept   0.00579   (0.953)
agepreg     0.00105   (0.783)
R^2 6.144e-06
\end{verbatim}

The parameter of {\tt agepreg} is positive, which suggests that
older mothers are more likely to have boys, but the p-value is
0.783, which means that the apparent effect could easily be due
to chance.
\index{p-value}
\index{age}

The coefficient of determination, $R^2$, does not apply to logistic
regression, but there are several alternatives that are used
as ``pseudo $R^2$ values.''  These values can be useful for comparing
models.  For example, here's a model that includes several factors
believed to be associated with sex ratio:
\index{model}
\index{coefficient of determination}
\index{r-squared}
\index{pseudo r-squared}

\begin{verbatim}
    formula = 'boy ~ agepreg + hpagelb + birthord + C(race)'
    model = smf.logit(formula, data=df)
    results = model.fit()
\end{verbatim}

Along with mother's age, this model includes father's age at
birth ({\tt hpagelb}), birth order ({\tt birthord}), and
race as a categorical variable.  Here are the results:
\index{categorical variable}

\begin{verbatim}
Intercept      -0.0301     (0.772)
C(race)[T.2]   -0.0224     (0.66)
C(race)[T.3]   -0.000457   (0.996)
agepreg        -0.00267    (0.629)
hpagelb         0.0047     (0.266)
birthord        0.00501    (0.821)
R^2 0.000144
\end{verbatim}

None of the estimated parameters are statistically significant.  The
pseudo-$R^2$ value is a little higher, but that could be due to
chance.
\index{pseudo r-squared}
  \index{significant} \index{statistically significant}


\section{Accuracy}
\label{accuracy}

In the office pool scenario,
we are most interested in the accuracy of the model:
the number of successful predictions, compared with what we would
expect by chance.
\index{model}
\index{accuracy}

In the NSFG data, there are more boys than girls, so the baseline
strategy is to guess ``boy'' every time.  The accuracy of this
strategy is just the fraction of boys:

\begin{verbatim}
    actual = endog['boy']
    baseline = actual.mean()
\end{verbatim}

Since {\tt actual} is encoded in binary integers, the mean is the
fraction of boys, which is 0.507.

Here's how we compute the accuracy of the model:

\begin{verbatim}
    predict = (results.predict() >= 0.5)
    true_pos = predict * actual
    true_neg = (1 - predict) * (1 - actual)
\end{verbatim}

{\tt results.predict} returns a NumPy array of probabilities, which we
round off to 0 or 1.  Multiplying by {\tt actual}
yields 1 if we predict a boy and get it right, 0 otherwise.  So,
\verb"true_pos" indicates ``true positives''.
\index{NumPy}
\index{true positive}
\index{true negative}

Similarly, \verb"true_neg" indicates the cases where we guess ``girl''
and get it right.  Accuracy is the fraction of correct guesses:

\begin{verbatim}
    acc = (sum(true_pos) + sum(true_neg)) / len(actual)
\end{verbatim}

The result is 0.512, slightly better than the
baseline, 0.507.  But, you should not take this result too seriously.
We used the same data to build and test the model, so the model
may not have predictive power on new data.
\index{model}

Nevertheless, let's use the model to make a prediction for the office
pool.  Suppose your friend is 35 years old and white,
her husband is 39, and they are expecting their third child:

\begin{verbatim}
    columns = ['agepreg', 'hpagelb', 'birthord', 'race']
    new = pandas.DataFrame([[35, 39, 3, 2]], columns=columns)
    y = results.predict(new)
\end{verbatim}

To invoke {\tt results.predict} for a new case, you have to construct
a DataFrame with a column for each variable in the model.  The result
in this case is 0.52, so you should guess ``boy.''  But if the model
improves your chances of winning, the difference is very small.
\index{DataFrame}



\section{Exercises}

My solution to these exercises is in \verb"chap11soln.ipynb".

\begin{exercise}
Suppose one of your co-workers is expecting a baby and you are
participating in an office pool to predict the date of birth.
Assuming that bets are placed during the 30th week of pregnancy, what
variables could you use to make the best prediction?  You should limit
yourself to variables that are known before the birth, and likely to
be available to the people in the pool.
\index{betting pool}
\index{date of birth}

\end{exercise}


\begin{exercise}
The Trivers-Willard hypothesis suggests that for many mammals the
sex ratio depends on ``maternal condition''; that is,
factors like the mother's age, size, health, and social status.
See \url{https://en.wikipedia.org/wiki/Trivers-Willard_hypothesis}
\index{Trivers-Willard hypothesis}
\index{sex ratio}

Some studies have shown this effect among humans, but results are
mixed.  In this chapter we tested some variables related to these
factors, but didn't find any with a statistically significant effect
on sex ratio.
  \index{significant} \index{statistically significant}

As an exercise, use a data mining approach to test the other variables
in the pregnancy and respondent files.  Can you find any factors with
a substantial effect?  
\index{data mining}

\end{exercise}


\begin{exercise}
If the quantity you want to predict is a count, you can use Poisson
regression, which is implemented in StatsModels with a function called
{\tt poisson}.  It works the same way as {\tt ols} and {\tt logit}.
As an exercise, let's use it to predict how many children a woman
has born; in the NSFG dataset, this variable is called {\tt numbabes}.
\index{StatsModels}
\index{Poisson regression}

Suppose you meet a woman who is 35 years old, black, and a college
graduate whose annual household income exceeds \$75,000.  How many
children would you predict she has born?
\end{exercise}


\begin{exercise}
If the quantity you want to predict is categorical, you can use
multinomial logistic regression, which is implemented in StatsModels
with a function called {\tt mnlogit}.  As an exercise, let's use it to
guess whether a woman is married, cohabitating, widowed, divorced,
separated, or never married; in the NSFG dataset, marital status is
encoded in a variable called {\tt rmarital}.
\index{categorical variable}
\index{marital status}

Suppose you meet a woman who is 25 years old, white, and a high
school graduate whose annual household income is about \$45,000.
What is the probability that she is married, cohabitating, etc?
\end{exercise}




\section{Glossary}

\begin{itemize}

\item regression: One of several related processes for estimating parameters
that fit a model to data.
\index{regression}

\item dependent variables: The variables in a regression model we would
like to predict.  Also known as endogenous variables.
\index{dependent variable}
\index{endogenous variable}

\item explanatory variables: The variables used to predict or explain
the dependent variables.  Also known as independent, or exogenous,
variables.
\index{explanatory variable}
\index{exogenous variable}

\item simple regression: A regression with only one dependent and
one explanatory variable.
\index{simple regression}

\item multiple regression: A regression with multiple explanatory
variables, but only one dependent variable.
\index{multiple regression}

\item linear regression: A regression based on a linear model.
\index{linear regression}

\item ordinary least squares: A linear regression that estimates
parameters by minimizing the squared error of the residuals.
\index{ordinary least squares}

\item spurious relationship: A relationship between two variables that is 
caused by a statistical artifact or a factor, not included in the
model, that is related to both variables.
\index{spurious relationship}

\item control variable: A variable included in a regression to
eliminate or ``control for'' a spurious relationship.
\index{control variable}

\item proxy variable: A variable that contributes information to
a regression model indirectly because of a relationship with another
factor, so it acts as a proxy for that factor.
\index{proxy variable}

\item categorical variable: A variable that can have one of a
discrete set of unordered values.
\index{categorical variable}

\item join: An operation that combines data from two DataFrames
using a key to match up rows in the two frames.
\index{join}
\index{DataFrame}

\item data mining: An approach to finding relationships between
variables by testing a large number of models.
\index{data mining}

\item logistic regression: A form of regression used when the
dependent variable is boolean.
\index{logistic regression}

\item Poisson regression: A form of regression used when the
dependent variable is a non-negative integer, usually a count.
\index{Poisson regression}

\item odds: An alternative way of representing a probability, $p$, as
  the ratio of the probability and its complement, $p / (1-p)$.
\index{odds}

\end{itemize}

