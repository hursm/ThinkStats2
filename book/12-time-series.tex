
\chapter{시계열 분석}

{\bf 시계열(time series)}은 시스템에서 시간에 따라 변화하는 측정값 시퀀스다. 
유명한 사례는 ``하키스틱 그래프 (hockey stick graph)''로 시간에 따른 글로벌 평균 기온을 보여준다.(\url{https://en.wikipedia.org/wiki/Hockey_stick_graph} 참조).
\index{시계열 (time series)}
\index{하키스틱 그래프 (hockey stick graph)}

이번 장에서 작업할 예제는 정치과학 연구자 Zachary M. Jones에게서 왔다. 
Zachary는 미국 대마초(마리화나)에 대한 암시장(black market)을 연구한다(\url{http://zmjones.com/marijuana}). ``담배 가격 (Price of Weed)''으로 불리는 웹사이트에서 데이터를 수집했다. 이 웹사이트는 대마초 거래장소, 가격, 품질, 수량을 참여자에게 물어 시장정보를 클라우드 소싱(crowd sourcing)했다(\url{http://www.priceofweed.com/}).
이 프로젝트의 목적은 사장에 대한 합법화(legalization)같은 정책결정 효과를 조사하는 것이다. 이 프로젝트에 매력을 발견했는데 이유는 데이터를 사용해서 중요한 정치적 질문 예를 들면, 약물정책(drug policy)같이 다루기 깨문이다.

\index{담배 가격 (Price of Weed)}
\index{대마초 (cannabis)}

이번 장에서 흥미를 찾았으면하는 희망이 있지만, 분석에 전문가적인 태도를 유지하는 중요성을 반복하는 기회가 되었으면 한다. 약물(drug)가 불법 혹은 어느 약물이 불법이 되어야 하느냐는 중요하고 어려운 공공정책질문이다; 정직하게 보고된 정확한 데이터로 우리의 결정을 통보해야 한다.

\index{윤리 (ethics)}

이번 장에서 사용되는 코드는 {\tt timeseries.py}에 있다.
코드를 다운로드하고 작업하는 것에 대한 정보는 ~\ref{code}을 참조한다.

\section{가져오기(Importing)와 정제하기(cleaning)}

Mr. Jones 사이트에서 다운로드한 데이터는 이책 저장소에 있다.
다음 코드가 데이터를 읽어 판다스 데이터프레임으로 저장한다.
\index{판다스 (pandas)}
\index{데이터프레임 (DataFrame)}

\begin{verbatim}
    transactions = pandas.read_csv('mj-clean.csv', parse_dates=[5])
\end{verbatim}

\verb"parse_dates"는 5번째 칼럼(열)값을 날짜 자료형으로 해석하고, 
넘파이(NumPy) {\tt datetime64} 객체로 전환한다.
\index{넘파이 (NumPy)}

데이터프레임에는 각각 보고된 거래건에 대한 행과 다음 칼럼(열)이 있다.

\begin{itemize}

\item city (도시): 문자열 도시 이름.

\item state (주): 알파벳 두 글자로된 미국 주명.

\item price (가격): 달러로 지불된 가격.
\index{가격 (price)}

\item amount (수량): 그램으로 구입한 수량.

\item quality (품질): 구매자가 보고한 고급, 보통, 저급 품질.

\item date (날짜): 보고날짜, 추측컨데 구매일 직후 날짜.

\item ppg: 달러 표기된 그램당 가격 (price per gram)

\item state.name: 문자열 미국 주 이름

\item lat: 도시 이름에 기반한 거래가 발생한 근사치 위도 정보.

\item lon: 거래가 발생한 근사치 위도 정보.

\end{itemize}



Each transaction is an event in time, so we could treat this dataset
as a time series.  But the events are not equally spaced in time; the
number of transactions reported each day varies from 0 to several
hundred.  Many methods used to analyze time series require the
measurements to be equally spaced, or at least things are simpler if
they are.
\index{transaction}
\index{equally spaced data}

In order to demonstrate these methods, I divide the dataset
into groups by reported quality, and then transform each group into
an equally spaced series by computing the mean daily price per gram.

\begin{verbatim}
def GroupByQualityAndDay(transactions):
    groups = transactions.groupby('quality')
    dailies = {}
    for name, group in groups:
        dailies[name] = GroupByDay(group)        

    return dailies
\end{verbatim}

{\tt groupby} is a DataFrame method that returns a GroupBy object,
{\tt groups}; used in a for loop, it iterates the names of the groups
and the DataFrames that represent them.  Since the values of {\tt
  quality} are {\tt low}, {\tt medium}, and {\tt high}, we get three
groups with those names.  \index{DataFrame} \index{groupby}

The loop iterates through the groups and calls {\tt GroupByDay},
which computes the daily average price and returns a new DataFrame:

\begin{verbatim}
def GroupByDay(transactions, func=np.mean):
    grouped = transactions[['date', 'ppg']].groupby('date')
    daily = grouped.aggregate(func)

    daily['date'] = daily.index
    start = daily.date[0]
    one_year = np.timedelta64(1, 'Y')
    daily['years'] = (daily.date - start) / one_year

    return daily
\end{verbatim}

The parameter, {\tt transactions}, is a DataFrame that contains
columns {\tt date} and {\tt ppg}.  We select these two
columns, then group by {\tt date}.
\index{groupby}

The result, {\tt grouped}, is a map from each date to a DataFrame that
contains prices reported on that date.  {\tt aggregate} is a
GroupBy method that iterates through the groups and applies a
function to each column of the group; in this case there is only one
column, {\tt ppg}.  So the result of {\tt aggregate} is a DataFrame
with one row for each date and one column, {\tt ppg}.
\index{aggregate}

Dates in these DataFrames are stored as NumPy {\tt datetime64}
objects, which are represented as 64-bit integers in nanoseconds.
For some of the analyses coming up, it will be convenient to
work with time in more human-friendly units, like years.  So
{\tt GroupByDay} adds a column named {\tt date} by copying
the {\tt index}, then adds {\tt years}, which contains the number
of years since the first transaction as a floating-point number.
\index{NumPy}
\index{datetime64}

The resulting DataFrame has columns {\tt ppg}, {\tt date}, and
{\tt years}.
\index{DataFrame}


\section{Plotting}

The result from {\tt GroupByQualityAndDay} is a map from each quality
to a DataFrame of daily prices.  Here's the code I use to plot
the three time series:
\index{DataFrame}
\index{visualization}

\begin{verbatim}
    thinkplot.PrePlot(rows=3)
    for i, (name, daily) in enumerate(dailies.items()):
        thinkplot.SubPlot(i+1)
        title = 'price per gram ($)' if i==0 else ''
        thinkplot.Config(ylim=[0, 20], title=title)
        thinkplot.Scatter(daily.index, daily.ppg, s=10, label=name)
        if i == 2: 
            pyplot.xticks(rotation=30)
        else:
            thinkplot.Config(xticks=[])
\end{verbatim}

{\tt PrePlot} with {\tt rows=3} means that we are planning to
make three subplots laid out in three rows.  The loop iterates
through the DataFrames and creates a scatter plot for each.  It is
common to plot time series with line segments between the points,
but in this case there are many data points and prices are highly
variable, so adding lines would not help.
\index{thinkplot}

Since the labels on the x-axis are dates, I use {\tt pyplot.xticks}
to rotate the ``ticks'' 30 degrees, making them more readable.
\index{pyplot}
\index{ticks}
\index{xticks}

\begin{figure}
% timeseries.py
\centerline{\includegraphics[width=3.5in]{figs/timeseries1.pdf}}
\caption{Time series of daily price per gram for high, medium, and low
quality cannabis.}
\label{timeseries1}
\end{figure}

Figure~\ref{timeseries1} shows the result.  One apparent feature in
these plots is a gap around November 2013.  It's possible that data
collection was not active during this time, or the data might not
be available.  We will consider ways to deal with this missing data
later.
\index{missing values}

Visually, it looks like the price of high quality cannabis is
declining during this period, and the price of medium quality is
increasing.  The price of low quality might also be increasing, but it
is harder to tell, since it seems to be more volatile.  Keep in mind
that quality data is reported by volunteers, so trends over time
might reflect changes in how participants apply these labels.
\index{price}


\section{Linear regression}
\label{timeregress}

Although there are methods specific to time series analysis, for many
problems a simple way to get started is by applying general-purpose
tools like linear regression.  The following function takes a
DataFrame of daily prices and computes a least squares fit, returning
the model and results objects from StatsModels:
\index{DataFrame}
\index{StatsModels}
\index{linear regression}

\begin{verbatim}
def RunLinearModel(daily):
    model = smf.ols('ppg ~ years', data=daily)
    results = model.fit()
    return model, results
\end{verbatim}

Then we can iterate through the qualities and fit a model to
each:

\begin{verbatim}
    for name, daily in dailies.items():
        model, results = RunLinearModel(daily)
        print(name)
        regression.SummarizeResults(results)
\end{verbatim}

Here are the results:

\begin{center}
\begin{tabular}{|l|l|l|c|} \hline
quality & intercept & slope & $R^2$ \\ \hline
high    & 13.450  & -0.708  & 0.444 \\
medium  &  8.879  & 0.283   & 0.050 \\
low     &  5.362  & 0.568   & 0.030 \\
\hline
\end{tabular}
\end{center}

The estimated slopes indicate that the price of high quality cannabis
dropped by about 71 cents per year during the observed interval; for
medium quality it increased by 28 cents per year, and for low quality
it increased by 57 cents per year.  These estimates are all
statistically significant with very small p-values.
\index{p-value}
  \index{significant} \index{statistically significant}

The $R^2$ value for high quality cannabis is 0.44, which means
that time as an explanatory variable accounts for 44\% of the observed
variability in price.  For the other qualities, the change in price
is smaller, and variability in prices is higher, so the values
of $R^2$ are smaller (but still statistically significant).
\index{explanatory variable}
  \index{significant} \index{statistically significant}

The following code plots the observed prices and the fitted values:

\begin{verbatim}
def PlotFittedValues(model, results, label=''):
    years = model.exog[:,1]
    values = model.endog
    thinkplot.Scatter(years, values, s=15, label=label)
    thinkplot.Plot(years, results.fittedvalues, label='model')
\end{verbatim}

As we saw in Section~\ref{implementation}, {\tt model} contains
{\tt exog} and {\tt endog}, NumPy arrays with the exogenous
(explanatory) and endogenous (dependent) variables.
\index{NumPy}
\index{explanatory variable}
\index{dependent variable}
\index{exogenous variable}
\index{endogenous variable}

\begin{figure}
% timeseries.py
\centerline{\includegraphics[height=2.5in]{figs/timeseries2.pdf}}
\caption{Time series of daily price per gram for high quality cannabis,
and a linear least squares fit.}
\label{timeseries2}
\end{figure}

{\tt PlotFittedValues} makes a scatter plot of the data points and a line
plot of the fitted values.  Figure~\ref{timeseries2} shows the results
for high quality cannabis.  The model seems like a good linear fit
for the data; nevertheless, linear regression is not the most 
appropriate choice for this data:
\index{model}
\index{fitted values}

\begin{itemize}

\item First, there is no reason to expect the long-term trend to be a
  line or any other simple function.  In general, prices are
  determined by supply and demand, both of which vary over time in
  unpredictable ways.
\index{trend}

\item Second, the linear regression model gives equal weight to all
  data, recent and past.  For purposes of prediction, we should
  probably give more weight to recent data.
\index{weight}

\item Finally, one of the assumptions of linear regression is that the
  residuals are uncorrelated noise.  With time series data, this
  assumption is often false because successive values are correlated.
\index{residuals}

\end{itemize}

The next section presents an alternative that is more appropriate
for time series data.


\section{Moving averages}

Most time series analysis is based on the modeling assumption that the
observed series is the sum of three components:
\index{model}
\index{moving average}

\begin{itemize}

\item Trend: A smooth function that captures persistent changes.
\index{trend}

\item Seasonality: Periodic variation, possibly including daily,
weekly, monthly, or yearly cycles.
\index{seasonality}

\item Noise: Random variation around the long-term trend.
\index{noise}

\end{itemize}

Regression is one way to extract the trend from a series, as we
saw in the previous section.  But if the trend is not a simple
function, a good alternative is a {\bf moving average}.  A moving
average divides the series into overlapping regions, called {\bf windows},
and computes the average of the values in each window.
\index{window}

One of the simplest moving averages is the {\bf rolling mean}, which
computes the mean of the values in each window.  For example, if
the window size is 3, the rolling mean computes the mean of
values 0 through 2, 1 through 3, 2 through 4, etc.
\index{rolling mean}
\index{mean!rolling}

pandas provides \verb"rolling_mean", which takes a Series and a
window size and returns a new Series.
\index{pandas}
\index{Series}

\begin{verbatim}
>>> series = np.arange(10)
array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])

>>> pandas.rolling_mean(series, 3)
array([ nan,  nan,   1,   2,   3,   4,   5,   6,   7,   8])
\end{verbatim}

The first two values are {\tt nan}; the next value is the mean of
the first three elements, 0, 1, and 2.  The next value is the mean
of 1, 2, and 3.  And so on.

Before we can apply \verb"rolling_mean" to the cannabis data, we
have to deal with missing values.  There are a few days in the
observed interval with no reported transactions for one or more
quality categories, and a period in 2013 when data collection was
not active.
\index{missing values}

In the DataFrames we have used so far, these dates are absent;
the index skips days with no data.  For the analysis that follows,
we need to represent this missing data explicitly.  We can do
that by ``reindexing'' the DataFrame:
 \index{DataFrame}
\index{reindex}

\begin{verbatim}
    dates = pandas.date_range(daily.index.min(), daily.index.max())
    reindexed = daily.reindex(dates)
\end{verbatim}

The first line computes a date range that includes every day from the
beginning to the end of the observed interval.  The second line
creates a new DataFrame with all of the data from {\tt daily}, but
including rows for all dates, filled with {\tt nan}.
\index{interval}
\index{date range}

Now we can plot the rolling mean like this:

\begin{verbatim}
    roll_mean = pandas.rolling_mean(reindexed.ppg, 30)
    thinkplot.Plot(roll_mean.index, roll_mean)
\end{verbatim}

The window size is 30, so each value in \verb"roll_mean" is
the mean of 30 values from {\tt reindexed.ppg}.  
\index{pandas}
\index{window}

\begin{figure}
% timeseries.py
\centerline{\includegraphics[height=2.5in]{figs/timeseries10.pdf}}
\caption{Daily price and a rolling mean (left) and exponentially-weighted
moving average (right).}
\label{timeseries10}
\end{figure}

Figure~\ref{timeseries10} (left)
shows the result. 
The rolling mean seems to do a good job of smoothing out the noise and
extracting the trend.  The first 29 values are {\tt nan}, and wherever
there's a missing value, it's followed by another 29 {\tt nan}s.
There are ways to fill in these gaps, but they are a minor nuisance.
\index{missing values}
\index{noise}
\index{smoothing}

An alternative is the {\bf exponentially-weighted moving average} (EWMA),
which has two advantages.  First, as the name suggests, it computes
a weighted average where the most recent value has the highest weight
and the weights for previous values drop off exponentially.
Second, the pandas implementation of EWMA handles missing values
better.
\index{reindex}
\index{exponentially-weighted moving average}
\index{EWMA}

\begin{verbatim}
    ewma = pandas.ewma(reindexed.ppg, span=30)
    thinkplot.Plot(ewma.index, ewma)
\end{verbatim}

The {\bf span} parameter corresponds roughly to the window size of
a moving average; it controls how fast the weights drop off, so it
determines the number of points that make a non-negligible contribution
to each average.
\index{span}
\index{window}

Figure~\ref{timeseries10} (right) shows the EWMA for the same data.
It is similar to the rolling mean, where they are both defined,
but it has no missing values, which makes it easier to work with.  The
values are noisy at the beginning of the time series, because they are
based on fewer data points.
\index{missing values}


\section{Missing values}

Now that we have characterized the trend of the time series, the
next step is to investigate seasonality, which is periodic behavior.
Time series data based on human behavior often exhibits daily,
weekly, monthly, or yearly cycles.  In the next section I present
methods to test for seasonality, but they don't work well with
missing data, so we have to solve that problem first.
\index{missing values}
\index{seasonality}

A simple and common way to fill missing data is to use a moving
average.  The Series method {\tt fillna} does just what we want:
\index{Series}
\index{fillna}

\begin{verbatim}
    reindexed.ppg.fillna(ewma, inplace=True)
\end{verbatim}

Wherever {\tt reindexed.ppg} is {\tt nan}, {\tt fillna} replaces
it with the corresponding value from {\tt ewma}.  The {\tt inplace}
flag tells {\tt fillna} to modify the existing Series rather than
create a new one.

A drawback of this method is that it understates the noise in the
series.  We can solve that problem by adding in resampled
residuals:
\index{resampling}
\index{noise}

\begin{verbatim}
    resid = (reindexed.ppg - ewma).dropna()
    fake_data = ewma + thinkstats2.Resample(resid, len(reindexed))
    reindexed.ppg.fillna(fake_data, inplace=True)
\end{verbatim}

% (One note on vocabulary: in this book I am using
%``resampling'' in the statistical sense, which is drawing a random
%sample from a population that is, itself, a sample.  In the context
%of time series analysis, it has another meaning: changing the
%time between measurements in a series.  I don't use the second
%meaning in this book, but you might encounter it.)

{\tt resid} contains the residual values, not including days
when {\tt ppg} is {\tt nan}.  \verb"fake_data" contains the
sum of the moving average and a random sample of residuals.
Finally, {\tt fillna} replaces {\tt nan} with values from
\verb"fake_data".
\index{dropna}
\index{fillna}
\index{NaN}

\begin{figure}
% timeseries.py
\centerline{\includegraphics[height=2.5in]{figs/timeseries8.pdf}}
\caption{Daily price with filled data.}
\label{timeseries8}
\end{figure}

Figure~\ref{timeseries8} shows the result.  The filled data is visually
similar to the actual values.  Since the resampled residuals are
random, the results are different every time; later we'll see how
to characterize the error created by missing values.
\index{resampling}
\index{missing values}


\section{Serial correlation}

As prices vary from day to day, you might expect to see patterns.
If the price is high on Monday,
you might expect it to be high for a few more days; and
if it's low, you might expect it to stay low.  A pattern
like this is called {\bf serial
correlation}, because each value is correlated with the next one
in the series.
\index{correlation!serial}
\index{serial correlation}

To compute serial correlation, we can shift the time series
by an interval called a {\bf lag}, and then compute the correlation
of the shifted series with the original:
\index{lag}

\begin{verbatim}
def SerialCorr(series, lag=1):
    xs = series[lag:]
    ys = series.shift(lag)[lag:]
    corr = thinkstats2.Corr(xs, ys)
    return corr
\end{verbatim}

After the shift, the first {\tt lag} values are {\tt nan}, so
I use a slice to remove them before computing {\tt Corr}.
\index{NaN}

%high 0.480121816154
%medium 0.164600078362
%low 0.103373620131

If we apply {\tt SerialCorr} to the raw price data with lag 1, we find
serial correlation 0.48 for the high quality category, 0.16 for
medium and 0.10 for low.  In any time series with a long-term trend,
we expect to see strong serial correlations; for example, if prices
are falling, we expect to see values above the mean in the first
half of the series and values below the mean in the second half.

It is more interesting to see if the correlation persists if you
subtract away the trend.  For example, we can compute the residual
of the EWMA and then compute its serial correlation:
\index{EWMA}

\begin{verbatim}
    ewma = pandas.ewma(reindexed.ppg, span=30)
    resid = reindexed.ppg - ewma
    corr = SerialCorr(resid, 1)
\end{verbatim}

With lag=1, the serial correlations for the de-trended data are
-0.022 for high quality, -0.015 for medium, and 0.036 for low.
These values are small, indicating that there is little or
no one-day serial correlation in this series.
\index{pandas}

To check for weekly, monthly, and yearly seasonality, I ran
the analysis again with different lags.  Here are the results:
\index{seasonality}

\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
lag & high & medium & low \\ \hline
1 & -0.029 & -0.014 & 0.034 \\
7 & 0.02 & -0.042 & -0.0097 \\
30 & 0.014 & -0.0064 & -0.013 \\
365 & 0.045 & 0.015 & 0.033 \\
\hline
\end{tabular}
\end{center}

In the next section we'll test whether these correlations are
statistically significant (they are not), but at this point we can
tentatively conclude that there are no substantial seasonal patterns
in these series, at least not with these lags.
  \index{significant} \index{statistically significant}


\section{Autocorrelation}

If you think a series might have some serial correlation, but you
don't know which lags to test, you can test them all!  The {\bf
  autocorrelation function} is a function that maps from lag to the
serial correlation with the given lag.  ``Autocorrelation'' is another
name for serial correlation, used more often when the lag is not 1.
\index{autocorrelation function}

StatsModels, which we used for linear regression in
Section~\ref{statsmodels}, also provides functions for time series
analysis, including {\tt acf}, which computes the autocorrelation
function:
\index{StatsModels}

\begin{verbatim}
    import statsmodels.tsa.stattools as smtsa
    acf = smtsa.acf(filled.resid, nlags=365, unbiased=True)
\end{verbatim}

{\tt acf} computes serial correlations with
lags from 0 through {\tt nlags}.  The {\tt unbiased} flag tells
{\tt acf} to correct the estimates for the sample size.  The result
is an array of correlations.  If we select daily prices for high
quality, and extract correlations for lags 1, 7, 30, and 365, we can
confirm that {\tt acf} and {\tt SerialCorr} yield approximately
the same results:
\index{acf}

\begin{verbatim}
>>> acf[0], acf[1], acf[7], acf[30], acf[365]
1.000, -0.029, 0.020, 0.014, 0.044
\end{verbatim}

With {\tt lag=0}, {\tt acf} computes the correlation of the series
with itself, which is always 1.
\index{lag}

\begin{figure}
% timeseries.py
\centerline{\includegraphics[height=2.5in]{figs/timeseries9.pdf}}
\caption{Autocorrelation function for daily prices (left), and
daily prices with a simulated weekly seasonality (right).}
\label{timeseries9}
\end{figure}

Figure~\ref{timeseries9} (left) shows autocorrelation functions for
the three quality categories, with {\tt nlags=40}.  The gray region
shows the normal variability we would expect if there is no actual
autocorrelation; anything that falls outside this range is
statistically significant, with a p-value less than 5\%.  Since
the false positive rate is 5\%, and
we are computing 120 correlations (40 lags for each of 3 times series),
we expect to see about 6 points outside this region.  In fact, there
are 7.  We conclude that there are no autocorrelations
in these series that could not be explained by chance.
\index{p-value}
  \index{significant} \index{statistically significant}
\index{false positive}

I computed the gray regions by resampling the residuals.  You
can see my code in {\tt timeseries.py}; the function is called
{\tt SimulateAutocorrelation}.
\index{resampling}

To see what the autocorrelation function looks like when there is a
seasonal component, I generated simulated data by adding a weekly
cycle.  Assuming that demand for cannabis is higher on weekends, we
might expect the price to be higher.  To simulate this effect, I
select dates that fall on Friday or Saturday and add a random amount
to the price, chosen from a uniform distribution from \$0 to \$2.
\index{simulation}
\index{uniform distribution}
\index{distribution!uniform}

\begin{verbatim}
def AddWeeklySeasonality(daily):
    frisat = (daily.index.dayofweek==4) | (daily.index.dayofweek==5)
    fake = daily.copy()
    fake.ppg[frisat] += np.random.uniform(0, 2, frisat.sum())
    return fake
\end{verbatim}

{\tt frisat} is a boolean Series, {\tt True} if the day of the
week is Friday or Saturday.  {\tt fake} is a new DataFrame, initially
a copy of {\tt daily}, which we modify by adding random values
to {\tt ppg}.  {\tt frisat.sum()} is the total number of Fridays
and Saturdays, which is the number of random values we have to
generate.
\index{DataFrame}
\index{Series}
\index{boolean}

Figure~\ref{timeseries9} (right) shows autocorrelation functions for
prices with this simulated seasonality.  As expected, the
correlations are highest when the lag is a multiple of 7.  For
high and medium quality, the new correlations are statistically
significant.  For low quality they are not, because residuals in this
category are large; the effect would have to be bigger
to be visible through the noise.
  \index{significant} \index{statistically significant}
\index{residuals}
\index{lag}


\section{Prediction}  

Time series analysis can be used to investigate, and sometimes
explain, the behavior of systems that vary in time.  It can also
make predictions.
\index{prediction}

The linear regressions we used in Section~\ref{timeregress} can be
used for prediction.  The RegressionResults class provides {\tt
  predict}, which takes a DataFrame containing the explanatory
variables and returns a sequence of predictions.  Here's the code:
\index{explanatory variable}
\index{linear regression}

\begin{verbatim}
def GenerateSimplePrediction(results, years):
    n = len(years)
    inter = np.ones(n)
    d = dict(Intercept=inter, years=years)
    predict_df = pandas.DataFrame(d)
    predict = results.predict(predict_df)
    return predict
\end{verbatim}

{\tt results} is a RegressionResults object; {\tt years} is the
sequence of time values we want predictions for.  The function
constructs a DataFrame, passes it to {\tt predict}, and
returns the result.
\index{pandas}
\index{DataFrame}

If all we want is a single, best-guess prediction, we're done.  But
for most purposes it is important to quantify error.  In other words,
we want to know how accurate the prediction is likely to be.

There are three sources of error we should take into account:

\begin{itemize}

\item Sampling error: The prediction is based on estimated
parameters, which depend on random variation
in the sample.  If we run the experiment again, we expect
the estimates to vary.
\index{sampling error}
\index{parameter}

\item Random variation:  Even if the estimated parameters are
perfect, the observed data varies randomly around the long-term
trend, and we expect this variation to continue in the future.
\index{noise}

\item Modeling error: We have already seen evidence that the long-term
trend is not linear, so predictions based on a linear model will
eventually fail.  
\index{modeling error}

\end{itemize}

Another source of error to consider is unexpected future events.
Agricultural prices are affected by weather, and all prices are
affected by politics and law.  As I write this, cannabis is legal in
two states and legal for medical purposes in 20 more.  If more states
legalize it, the price is likely to go down.  But if
the federal government cracks down, the price might go up.

Modeling errors and unexpected future events are hard to quantify.
Sampling error and random variation are easier to deal with, so we'll
do that first.

To quantify sampling error, I use resampling, as we did in
Section~\ref{regest}.  As always, the goal is to use the actual
observations to simulate what would happen if we ran the experiment
again.  The simulations are based on the assumption that the estimated
parameters are correct, but the random residuals could have been
different.  Here is a function that runs the simulations:
\index{resampling}

\begin{verbatim}
def SimulateResults(daily, iters=101):
    model, results = RunLinearModel(daily)
    fake = daily.copy()
    
    result_seq = []
    for i in range(iters):
        fake.ppg = results.fittedvalues + Resample(results.resid)
        _, fake_results = RunLinearModel(fake)
        result_seq.append(fake_results)

    return result_seq
\end{verbatim}

{\tt daily} is a DataFrame containing the observed prices;
{\tt iters} is the number of simulations to run.
\index{DataFrame}
\index{price}

{\tt SimulateResults} uses {\tt RunLinearModel}, from
Section~\ref{timeregress}, to estimate the slope and intercept
of the observed values.

Each time through the loop, it generates a ``fake'' dataset by
resampling the residuals and adding them to the fitted values.  Then
it runs a linear model on the fake data and stores the RegressionResults
object.
\index{model}
\index{residuals}

The next step is to use the simulated results to generate predictions:

\begin{verbatim}
def GeneratePredictions(result_seq, years, add_resid=False):
    n = len(years)
    d = dict(Intercept=np.ones(n), years=years, years2=years**2)
    predict_df = pandas.DataFrame(d)
    
    predict_seq = []
    for fake_results in result_seq:
        predict = fake_results.predict(predict_df)
        if add_resid:
            predict += thinkstats2.Resample(fake_results.resid, n)
        predict_seq.append(predict)

    return predict_seq
\end{verbatim}

{\tt GeneratePredictions} takes the sequence of results from the
previous step, as well as {\tt years}, which is a sequence of
floats that specifies the interval to generate predictions for,
and \verb"add_resid", which indicates whether it should add resampled
residuals to the straight-line prediction.
{\tt GeneratePredictions} iterates through the sequence of
RegressionResults and generates a sequence of predictions.
\index{resampling}

\begin{figure}
% timeseries.py
\centerline{\includegraphics[height=2.5in]{figs/timeseries4.pdf}}
\caption{Predictions based on linear fits, showing variation due
to sampling error and prediction error.}
\label{timeseries4}
\end{figure}

Finally, here's the code that plots a 90\% confidence interval for
the predictions:
\index{confidence interval}

\begin{verbatim}
def PlotPredictions(daily, years, iters=101, percent=90):
    result_seq = SimulateResults(daily, iters=iters)
    p = (100 - percent) / 2
    percents = p, 100-p

    predict_seq = GeneratePredictions(result_seq, years, True)
    low, high = thinkstats2.PercentileRows(predict_seq, percents)
    thinkplot.FillBetween(years, low, high, alpha=0.3, color='gray')

    predict_seq = GeneratePredictions(result_seq, years, False)
    low, high = thinkstats2.PercentileRows(predict_seq, percents)
    thinkplot.FillBetween(years, low, high, alpha=0.5, color='gray')
\end{verbatim}

{\tt PlotPredictions} calls {\tt GeneratePredictions} twice: once
with \verb"add_resid=True" and again with \verb"add_resid=False".
It uses {\tt PercentileRows} to select the 5th and 95th percentiles
for each year, then plots a gray region between these bounds.
\index{FillBetween}

Figure~\ref{timeseries4} shows the result.
The dark gray region represents a 90\% confidence interval for
the sampling error; that is, uncertainty about the estimated
slope and intercept due to sampling.
\index{sampling error}

The lighter region shows
a 90\% confidence interval for prediction error, which is the
sum of sampling error and random variation.
\index{noise}

These regions quantify sampling error and random variation, but
not modeling error.  In general modeling error is hard to quantify,
but in this case we can address at least one source of error,
unpredictable external events.
\index{modeling error}

The regression model is based on the assumption that the system
is {\bf stationary}; that is, that the parameters of the model
don't change over time.
Specifically, it assumes that the slope and
intercept are constant, as well as the distribution of residuals.
\index{stationary model}
\index{parameter}

But looking at the moving averages in Figure~\ref{timeseries10}, it
seems like the slope changes at least once during the observed
interval, and the variance of the residuals seems bigger in the first
half than the second.
\index{slope}

As a result, the parameters we get depend on the interval we
observe.  To see how much effect this has on the predictions,
we can extend {\tt SimulateResults} to use intervals of observation
with different start and end dates.  My implementation is in
{\tt timeseries.py}.
\index{simulation}

\begin{figure}
% timeseries.py
\centerline{\includegraphics[height=2.5in]{figs/timeseries5.pdf}}
\caption{Predictions based on linear fits, showing
variation due to the interval of observation.}
\label{timeseries5}
\end{figure}

Figure~\ref{timeseries5} shows the result for the medium quality
category.  The lightest gray area shows a confidence interval that
includes uncertainty due to sampling error, random variation, and
variation in the interval of observation.
\index{confidence interval}
\index{interval}

The model based on the entire interval has positive slope, indicating
that prices were increasing.  But the most recent interval shows signs
of decreasing prices, so models based on the most recent data have
negative slope.  As a result, the widest predictive interval includes
the possibility of decreasing prices over the next year.
\index{model}


\section{Further reading}

Time series analysis is a big topic; this chapter has only scratched
the surface.  An important tool for working with time series data
is autoregression, which I did not cover here, mostly because it turns
out not to be useful for the example data I worked with.
\index{time series}

But once you
have learned the material in this chapter, you are well prepared
to learn about autoregression.  One resource I recommend is
Philipp Janert's book, {\it Data Analysis with Open Source Tools},
O'Reilly Media, 2011.  His chapter on time series analysis picks up
where this one leaves off.
\index{Janert, Philipp}


\section{Exercises}

My solution to these exercises is in \verb"chap12soln.py".

\begin{exercise}
The linear model I used in this chapter has the obvious drawback
that it is linear, and there is no reason to expect prices to
change linearly over time.
We can add flexibility to the model by adding a quadratic term,
as we did in Section~\ref{nonlinear}.  
\index{nonlinear}
\index{linear model}
\index{quadratic model}

Use a quadratic model to fit the time series of daily prices,
and use the model to generate predictions.  You will have to
write a version of {\tt RunLinearModel} that runs that quadratic
model, but after that you should be able to reuse code in
{\tt timeseries.py} to generate predictions.
\index{prediction}

\end{exercise}

\begin{exercise}
Write a definition for a class named {\tt SerialCorrelationTest}
that extends {\tt HypothesisTest} from Section~\ref{hypotest}.
It should take a series and a lag as data, compute the serial
correlation of the series with the given lag, and then compute
the p-value of the observed correlation.
\index{HypothesisTest}
\index{p-value}
\index{lag}

Use this class to test whether the serial correlation in raw
price data is statistically significant.  Also test the residuals
of the linear model and (if you did the previous exercise),
the quadratic model.
\index{quadratic model}
  \index{significant} \index{statistically significant}

\end{exercise}

\begin{exercise}
There are several ways to extend the EWMA model to generate predictions.
One of the simplest is something like this:
\index{EWMA}

\begin{enumerate}

\item Compute the EWMA of the time series and use the last point
as an intercept, {\tt inter}.

\item Compute the EWMA of differences between successive elements in
the time series and use the last point as a slope, {\tt slope}.
\index{slope}

\item To predict values at future times, compute {\tt inter + slope * dt},
where {\tt dt} is the difference between the time of the prediction and
the time of the last observation.
\index{prediction}

\end{enumerate}

Use this method to generate predictions for a year after the last
observation.  A few hints:

\begin{itemize}

\item Use {\tt timeseries.FillMissing} to fill in missing values
before running this analysis.  That way the time between consecutive
elements is consistent.
\index{missing values}

\item Use {\tt Series.diff} to compute differences between successive
elements.
\index{Series}

\item Use {\tt reindex} to extend the DataFrame index into the future.
\index{reindex}

\item Use {\tt fillna} to put your predicted values into the DataFrame.
\index{fillna}

\end{itemize}

\end{exercise}


\section{Glossary}

\begin{itemize}

\item time series: A dataset where each value is associated with
a timestamp, often a series of measurements and the times they
were collected.
\index{time series}

\item window: A sequence of consecutive values in a time series,
often used to compute a moving average.
\index{window}

\item moving average: One of several statistics intended to estimate
the underlying trend in a time series by computing averages (of
some kind) for a series of overlapping windows.
\index{moving average}

\item rolling mean: A moving average based on the mean value in
each window.
\index{rolling mean}

\item exponentially-weighted moving average (EWMA): A moving
average based on a weighted mean that gives the highest weight
to the most recent values, and exponentially decreasing weights
to earlier values.
\index{exponentially-weighted moving average}
\index{EWMA}

\item span: A parameter of EWMA that determines how quickly the
weights decrease.
\index{span}

\item serial correlation: Correlation between a time series and
a shifted or lagged version of itself.
\index{serial correlation}

\item lag: The size of the shift in a serial correlation or
autocorrelation.
\index{lag}

\item autocorrelation: A more general term for a serial correlation
with any amount of lag.
\index{autocorrelation function}

\item autocorrelation function: A function that maps from lag to
serial correlation.

\item stationary: A model is stationary if the parameters and the
distribution of residuals does not change over time.
\index{model}
\index{stationary model}

\end{itemize}

